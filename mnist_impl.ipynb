{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "from IPython.core.debugger import set_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipynb_reader\n",
    "import mnist_load as load\n",
    "\n",
    "#defining data \n",
    "train_images_array = load.train_images_array;\n",
    "train_labels_array = load.train_labels_array;\n",
    "test_images_array = load.test_images_array;\n",
    "test_labels_array = load.test_labels_array;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images_array = train_images_array.reshape((train_images_array.shape[0], -1))\n",
    "test_images_array = test_images_array.reshape((test_images_array.shape[0], -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simplest implementation - Two layer neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = train_images_array\n",
    "goal_pred = train_labels_array\n",
    "weights = np.zeros((input.shape[1], goal_pred.shape[1]))\n",
    "iterations, alpha = 100, .01\n",
    "\n",
    "def neural_network(input, weights): \n",
    "    for i in range(iterations):\n",
    "        pred = input.dot(weights)\n",
    "        delta = pred - goal_pred\n",
    "        error = np.sum((delta ** 2)) / input.shape[0]\n",
    "        weight_delta = delta.T.dot(input).T\n",
    "        weights -= weight_delta * alpha\n",
    "        print(' Error: ' + str(error))\n",
    "        \n",
    "neural_network(input, weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Three layer neural network - Nonlinearity and backpropagation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train err: 0.8288112117657275 Train acc: 326 Test err: 0.7739978581822734Test acc: 337\n",
      "Train err: 0.3052834866693787 Train acc: 857 Test err: 0.5574162363545417Test acc: 641\n",
      "Train err: 0.2151555808615371 Train acc: 912 Test err: 0.5006069547010612Test acc: 713\n",
      "Train err: 0.1731913840373818 Train acc: 934 Test err: 0.48507880715910784Test acc: 734\n",
      "Train err: 0.14830335106149323 Train acc: 951 Test err: 0.4743294954691758Test acc: 735\n",
      "Train err: 0.1293786979814382 Train acc: 961 Test err: 0.47386161318692216Test acc: 754\n",
      "Train err: 0.11452623769554238 Train acc: 966 Test err: 0.46489997488910384Test acc: 763\n",
      "Train err: 0.10282767254153001 Train acc: 970 Test err: 0.45471101068323333Test acc: 774\n",
      "Train err: 0.09412530429867552 Train acc: 972 Test err: 0.44942717047693626Test acc: 772\n",
      "Train err: 0.08733014543876857 Train acc: 974 Test err: 0.45083337256342976Test acc: 767\n",
      "Train err: 0.1054236640373662 Train acc: 957 Test err: 0.4512706436225698Test acc: 765\n",
      "Train err: 0.07426604516798402 Train acc: 980 Test err: 0.43321259396995077Test acc: 766\n",
      "Train err: 0.07143755365718017 Train acc: 981 Test err: 0.42169672327304186Test acc: 772\n",
      "Train err: 0.0664046895626987 Train acc: 981 Test err: 0.4264796830584312Test acc: 774\n",
      "Train err: 0.06372748873478691 Train acc: 981 Test err: 0.4350991447358183Test acc: 763\n",
      "Train err: 0.061611043086590093 Train acc: 981 Test err: 0.4246745315907735Test acc: 759\n",
      "Train err: 0.05706809384397892 Train acc: 981 Test err: 0.4178487690560489Test acc: 771\n",
      "Train err: 0.054424773120831896 Train acc: 981 Test err: 0.42222695900929164Test acc: 767\n",
      "Train err: 0.05211050183777086 Train acc: 981 Test err: 0.4257173220094971Test acc: 775\n",
      "Train err: 0.05006205948718531 Train acc: 981 Test err: 0.42573253555407325Test acc: 774\n",
      "Train err: 0.049796372464548905 Train acc: 982 Test err: 0.42938853231005264Test acc: 764\n",
      "Train err: 0.04678784429967696 Train acc: 982 Test err: 0.442202438495675Test acc: 761\n",
      "Train err: 0.04644260177339687 Train acc: 982 Test err: 0.4512962833462851Test acc: 761\n",
      "Train err: 0.046418752758872264 Train acc: 982 Test err: 0.4572963096187898Test acc: 747\n",
      "Train err: 0.04433054334526541 Train acc: 982 Test err: 0.4506923352233131Test acc: 758\n",
      "Train err: 0.04341884966194739 Train acc: 982 Test err: 0.45162452326970387Test acc: 757\n",
      "Train err: 0.04198214738279796 Train acc: 982 Test err: 0.4473999766227879Test acc: 762\n",
      "Train err: 0.040307257315064515 Train acc: 983 Test err: 0.446156632893096Test acc: 765\n",
      "Train err: 0.04006037245163724 Train acc: 983 Test err: 0.4433699388746335Test acc: 763\n",
      "Train err: 0.038657435281624394 Train acc: 983 Test err: 0.44502994826191Test acc: 762\n",
      "Train err: 0.040425927022822196 Train acc: 982 Test err: 0.4409316542416353Test acc: 753\n",
      "Train err: 0.03779639817564995 Train acc: 983 Test err: 0.4400407394143389Test acc: 759\n",
      "Train err: 0.03764912265335601 Train acc: 983 Test err: 0.4356456243453012Test acc: 759\n",
      "Train err: 0.03914641569324484 Train acc: 982 Test err: 0.4378477888261753Test acc: 759\n",
      "Train err: 0.03481420732863084 Train acc: 983 Test err: 0.44059895024243245Test acc: 752\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "iterations, alpha = 350, .01\n",
    "\n",
    "hidden_size = 30\n",
    "\n",
    "weights_0_1 = .2 * np.random.random((train_images_array.shape[1], hidden_size)) - .1\n",
    "weights_1_2 = .2 * np.random.random((hidden_size, train_labels_array.shape[1])) - .1\n",
    "\n",
    "weights = {\n",
    "    'weights_0_1': weights_0_1,\n",
    "    'weights_1_2': weights_1_2\n",
    "}\n",
    "\n",
    "def relu(x):\n",
    "    return (x > 0) * x\n",
    "\n",
    "def relu2deriv(x): \n",
    "    return (x > 0)\n",
    "\n",
    "\n",
    "def neural_network(input: np.ndarray, weights: dict) -> None:\n",
    "    for iteration in range(iterations):\n",
    "        train_error = 0\n",
    "        train_correct_cnt = 0\n",
    "        for index, observation in enumerate(input):\n",
    "            #forward propagation\n",
    "            layer_0 = input[index:index+1]\n",
    "            layer_1 = relu(layer_0.dot(weights['weights_0_1']))\n",
    "            layer_2 = layer_1.dot(weights['weights_1_2'])\n",
    "            \n",
    "            layer_2_delta = layer_2 - train_labels_array[index:index+1]\n",
    "            \n",
    "            train_error += np.sum((layer_2_delta ** 2))\n",
    "            train_correct_cnt += int(np.argmax(layer_2) == np.argmax(train_labels_array[index:index+1]))\n",
    "            \n",
    "            layer_1_delta = layer_2_delta.dot(weights['weights_1_2'].T) \\\n",
    "                                * relu2deriv(layer_1)\n",
    "            \n",
    "            weights_1_2_delta = layer_1.T.dot(layer_2_delta)\n",
    "            weights_0_1_delta = layer_0.T.dot(layer_1_delta)\n",
    "            \n",
    "            weights['weights_1_2'] -= weights_1_2_delta * alpha\n",
    "            weights['weights_0_1'] -= weights_0_1_delta * alpha\n",
    "            \n",
    "        \n",
    "        if(iteration % 10 == 0):\n",
    "            test_error = 0\n",
    "            test_correct_cnt = 0\n",
    "            for index, observation in enumerate(test_images_array):\n",
    "                #forward propagation\n",
    "                layer_0 = test_images_array[index:index+1]\n",
    "                layer_1 = relu(layer_0.dot(weights['weights_0_1']))\n",
    "                layer_2 = layer_1.dot(weights['weights_1_2'])\n",
    "\n",
    "                layer_2_delta = layer_2 - test_labels_array[index:index+1]\n",
    "\n",
    "                test_error += np.sum((layer_2_delta ** 2))\n",
    "                test_correct_cnt += int(np.argmax(layer_2) == np.argmax(test_labels_array[index:index+1]))\n",
    "\n",
    "            print('Train err: ' + str(train_error/len(train_images_array)) + ' Train acc: ' + str(train_correct_cnt) \\\n",
    "                      + ' Test err: ' + str(test_error/len(test_images_array)) + 'Test acc: ' + str(test_correct_cnt))\n",
    "            \n",
    "            \n",
    "neural_network(train_images_array, weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Add dropout**\n",
    "\n",
    "This should point out to what dropout really is: it's noise. It makes it more difficult for the network to train on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train err: 0.9250876349606809 Train acc: 265 Test err: 0.7831812858165159 Test acc: 432\n",
      "Train err: 0.5569089972691266 Train acc: 637 Test err: 0.5348375824714446 Test acc: 665\n",
      "Train err: 0.49294010009549516 Train acc: 697 Test err: 0.49867365412301534 Test acc: 683\n",
      "Train err: 0.4348550132879813 Train acc: 761 Test err: 0.4329321580006654 Test acc: 760\n",
      "Train err: 0.3991034824374928 Train acc: 793 Test err: 0.4052929193195025 Test acc: 777\n",
      "Train err: 0.3910061741624961 Train acc: 811 Test err: 0.42118278966625483 Test acc: 782\n",
      "Train err: 0.379394629428567 Train acc: 807 Test err: 0.37356590907920745 Test acc: 807\n",
      "Train err: 0.36493899436383537 Train acc: 839 Test err: 0.4030171396893566 Test acc: 811\n",
      "Train err: 0.3659492229094198 Train acc: 841 Test err: 0.3687700744381114 Test acc: 802\n",
      "Train err: 0.35500889780196093 Train acc: 853 Test err: 0.3639346974165983 Test acc: 815\n",
      "Train err: 0.3434234782673846 Train acc: 841 Test err: 0.3743493231847987 Test acc: 799\n",
      "Train err: 0.3445611515254711 Train acc: 854 Test err: 0.3785329876233663 Test acc: 783\n",
      "Train err: 0.3337597053881207 Train acc: 857 Test err: 0.3609437021779664 Test acc: 819\n",
      "Train err: 0.344458417109497 Train acc: 860 Test err: 0.3784781538553013 Test acc: 796\n",
      "Train err: 0.3391606653382765 Train acc: 854 Test err: 0.3461455898861828 Test acc: 814\n",
      "Train err: 0.3426069345975841 Train acc: 840 Test err: 0.3655152978008105 Test acc: 814\n",
      "Train err: 0.3204809420490901 Train acc: 866 Test err: 0.3645017765645619 Test acc: 825\n",
      "Train err: 0.322612901163109 Train acc: 865 Test err: 0.3683410249713503 Test acc: 813\n",
      "Train err: 0.3107573594291265 Train acc: 879 Test err: 0.32832263896066255 Test acc: 822\n",
      "Train err: 0.3183565885092059 Train acc: 880 Test err: 0.368799210621943 Test acc: 798\n",
      "Train err: 0.30819248505792013 Train acc: 873 Test err: 0.3501025879786545 Test acc: 838\n",
      "Train err: 0.31285310854782056 Train acc: 878 Test err: 0.3590940217285954 Test acc: 801\n",
      "Train err: 0.31587931291571497 Train acc: 873 Test err: 0.34074100049905554 Test acc: 824\n",
      "Train err: 0.31433103685728053 Train acc: 885 Test err: 0.3699818285582161 Test acc: 804\n",
      "Train err: 0.31875542940225743 Train acc: 876 Test err: 0.33195608848258523 Test acc: 824\n",
      "Train err: 0.3239615874838209 Train acc: 867 Test err: 0.36313884356694154 Test acc: 796\n",
      "Train err: 0.3058273003947897 Train acc: 889 Test err: 0.3490141705280541 Test acc: 826\n",
      "Train err: 0.31347740330882923 Train acc: 883 Test err: 0.3349738807720512 Test acc: 824\n",
      "Train err: 0.3036407256590503 Train acc: 882 Test err: 0.3495700105198646 Test acc: 819\n",
      "Train err: 0.311055711287468 Train acc: 881 Test err: 0.35324964671522396 Test acc: 813\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "iterations, alpha = 300, .005\n",
    "\n",
    "hidden_size = 100\n",
    "\n",
    "weights_0_1 = .2 * np.random.random((train_images_array.shape[1], hidden_size)) - .1\n",
    "weights_1_2 = .2 * np.random.random((hidden_size, train_labels_array.shape[1])) - .1\n",
    "\n",
    "weights = {\n",
    "    'weights_0_1': weights_0_1,\n",
    "    'weights_1_2': weights_1_2\n",
    "}\n",
    "\n",
    "def relu(x):\n",
    "    return (x > 0) * x\n",
    "\n",
    "def relu2deriv(x): \n",
    "    return (x > 0)\n",
    "\n",
    "\n",
    "def neural_network(input: np.ndarray, weights: dict) -> None:\n",
    "    for iteration in range(iterations):\n",
    "        train_error = 0\n",
    "        train_correct_cnt = 0\n",
    "        for index, observation in enumerate(input):\n",
    "            #forward propagation\n",
    "            layer_0 = input[index:index+1]            \n",
    "            \n",
    "            layer_1 = relu(layer_0.dot(weights['weights_0_1']))\n",
    "            dropout_mask = np.random.randint(2, size=layer_1.shape)\n",
    "            layer_1 *= dropout_mask * 2\n",
    "            \n",
    "            layer_2 = layer_1.dot(weights['weights_1_2'])\n",
    "            \n",
    "            layer_2_delta = layer_2 - train_labels_array[index:index+1]\n",
    "            \n",
    "            train_error += np.sum((layer_2_delta ** 2))\n",
    "            train_correct_cnt += int(np.argmax(layer_2) == np.argmax(train_labels_array[index:index+1]))\n",
    "            \n",
    "            layer_1_delta = layer_2_delta.dot(weights['weights_1_2'].T) \\\n",
    "                                * relu2deriv(layer_1)\n",
    "            layer_1_delta *= dropout_mask\n",
    "            \n",
    "            weights_1_2_delta = layer_1.T.dot(layer_2_delta)\n",
    "            weights_0_1_delta = layer_0.T.dot(layer_1_delta)\n",
    "            \n",
    "            weights['weights_1_2'] -= weights_1_2_delta * alpha\n",
    "            weights['weights_0_1'] -= weights_0_1_delta * alpha\n",
    "            \n",
    "        \n",
    "        if(iteration % 10 == 0):\n",
    "            test_error = 0\n",
    "            test_correct_cnt = 0\n",
    "            for index, observation in enumerate(test_images_array):\n",
    "                #forward propagation\n",
    "                layer_0 = test_images_array[index:index+1]\n",
    "                layer_1 = relu(layer_0.dot(weights['weights_0_1']))\n",
    "                layer_2 = layer_1.dot(weights['weights_1_2'])\n",
    "\n",
    "                layer_2_delta = layer_2 - test_labels_array[index:index+1]\n",
    "\n",
    "                test_error += np.sum((layer_2_delta ** 2))\n",
    "                test_correct_cnt += int(np.argmax(layer_2) == np.argmax(test_labels_array[index:index+1]))\n",
    "\n",
    "            print('Train err: ' + str(train_error/len(train_images_array)) + ' Train acc: ' + str(train_correct_cnt) \\\n",
    "                      + ' Test err: ' + str(test_error/len(test_images_array)) + ' Test acc: ' + str(test_correct_cnt))\n",
    "            \n",
    "            \n",
    "neural_network(train_images_array, weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Add Batch gradient descent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train err: 0.0005573970698997744 Train acc: 141 Test err: 0.9830041578389545 Test acc: 209\n",
      "Train err: 8.189187596183152e-05 Train acc: 301 Test err: 0.8055807344912501 Test acc: 291\n",
      "Train err: 7.554899178711649e-05 Train acc: 405 Test err: 0.7293137678964844 Test acc: 481\n",
      "Train err: 7.204728741796208e-05 Train acc: 417 Test err: 0.6474245415902079 Test acc: 538\n",
      "Train err: 6.935513674738765e-05 Train acc: 437 Test err: 0.6254958069504352 Test acc: 584\n",
      "Train err: 6.742269984629588e-05 Train acc: 460 Test err: 0.6199360940979987 Test acc: 604\n",
      "Train err: 6.653739280397698e-05 Train acc: 477 Test err: 0.5900802644333478 Test acc: 632\n",
      "Train err: 6.62595075372212e-05 Train acc: 467 Test err: 0.5926669787746321 Test acc: 699\n",
      "Train err: 6.384734976636664e-05 Train acc: 503 Test err: 0.5922647776910404 Test acc: 649\n",
      "Train err: 6.177237911438533e-05 Train acc: 521 Test err: 0.5607088685792284 Test acc: 728\n",
      "Train err: 5.971055902248943e-05 Train acc: 541 Test err: 0.522805397067298 Test acc: 732\n",
      "Train err: 5.966062169715221e-05 Train acc: 526 Test err: 0.5218061600640301 Test acc: 737\n",
      "Train err: 5.507251573204988e-05 Train acc: 567 Test err: 0.5216891643455006 Test acc: 702\n",
      "Train err: 5.7585449036881136e-05 Train acc: 546 Test err: 0.499822074184158 Test acc: 727\n",
      "Train err: 6.166948548054233e-05 Train acc: 517 Test err: 0.5169321658651638 Test acc: 772\n",
      "Train err: 5.68310487899491e-05 Train acc: 558 Test err: 0.49123143793879376 Test acc: 786\n",
      "Train err: 5.640074376083894e-05 Train acc: 551 Test err: 0.48361107264086517 Test acc: 757\n",
      "Train err: 5.5900072453822466e-05 Train acc: 558 Test err: 0.48653202259441886 Test acc: 765\n",
      "Train err: 5.4494196138338634e-05 Train acc: 566 Test err: 0.5035976435800342 Test acc: 755\n",
      "Train err: 5.53859398791341e-05 Train acc: 570 Test err: 0.47656213616425486 Test acc: 774\n",
      "Train err: 5.498406678855424e-05 Train acc: 575 Test err: 0.4901032480599877 Test acc: 759\n",
      "Train err: 5.473063482095325e-05 Train acc: 572 Test err: 0.4784090661015449 Test acc: 771\n",
      "Train err: 5.385368031732647e-05 Train acc: 559 Test err: 0.4646258457524753 Test acc: 757\n",
      "Train err: 5.190512352098696e-05 Train acc: 613 Test err: 0.47249705366525313 Test acc: 761\n",
      "Train err: 5.214126084001347e-05 Train acc: 601 Test err: 0.4746626485610316 Test acc: 763\n",
      "Train err: 5.2497580392983134e-05 Train acc: 592 Test err: 0.47442500343659333 Test acc: 766\n",
      "Train err: 5.1395989306665484e-05 Train acc: 604 Test err: 0.4727039057689761 Test acc: 764\n",
      "Train err: 5.178045219569056e-05 Train acc: 614 Test err: 0.4564774057144735 Test acc: 774\n",
      "Train err: 5.3107101827426555e-05 Train acc: 591 Test err: 0.45266401156655806 Test acc: 773\n",
      "Train err: 4.7501395748544576e-05 Train acc: 651 Test err: 0.4516109963575228 Test acc: 764\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "iterations, alpha = 300, .1\n",
    "\n",
    "hidden_size, batch_size = 100, 100\n",
    "\n",
    "weights_0_1 = .2 * np.random.random((train_images_array.shape[1], hidden_size)) - .1\n",
    "weights_1_2 = .2 * np.random.random((hidden_size, train_labels_array.shape[1])) - .1\n",
    "\n",
    "weights = {\n",
    "    'weights_0_1': weights_0_1,\n",
    "    'weights_1_2': weights_1_2\n",
    "}\n",
    "\n",
    "def relu(x):\n",
    "    return (x > 0) * x\n",
    "\n",
    "def relu2deriv(x): \n",
    "    return (x > 0)\n",
    "\n",
    "\n",
    "def neural_network(input: np.ndarray, weights: dict) -> None:\n",
    "    for iteration in range(iterations):\n",
    "        train_error = 0\n",
    "        train_correct_cnt = 0\n",
    "        for index in range(int(len(train_images_array)/batch_size)):\n",
    "            \n",
    "            #batch\n",
    "            batch_start, batch_end = index * batch_size, (index + 1) * batch_size\n",
    "            \n",
    "            #forward propagation\n",
    "            layer_0 = input[batch_start:batch_end]            \n",
    "            \n",
    "            layer_1 = relu(layer_0.dot(weights['weights_0_1']))\n",
    "            dropout_mask = np.random.randint(2, size=layer_1.shape)\n",
    "            layer_1 *= dropout_mask * 2\n",
    "            \n",
    "            layer_2 = layer_1.dot(weights['weights_1_2'])\n",
    "            \n",
    "            layer_2_delta = (layer_2 - train_labels_array[batch_start:batch_end]) / batch_size\n",
    "            \n",
    "            train_error += np.sum((layer_2_delta ** 2))\n",
    "            train_correct_cnt += np.sum((np.argmax(layer_2, axis=1) == np.argmax(train_labels_array[batch_start:batch_end], axis=1)).astype(int))\n",
    "            \n",
    "            layer_1_delta = layer_2_delta.dot(weights['weights_1_2'].T) \\\n",
    "                                * relu2deriv(layer_1)\n",
    "            layer_1_delta *= dropout_mask\n",
    "            \n",
    "            weights_1_2_delta = layer_1.T.dot(layer_2_delta)\n",
    "            weights_0_1_delta = layer_0.T.dot(layer_1_delta)\n",
    "            \n",
    "            weights['weights_1_2'] -= weights_1_2_delta * alpha\n",
    "            weights['weights_0_1'] -= weights_0_1_delta * alpha\n",
    "            \n",
    "        \n",
    "        if(iteration % 10 == 0):\n",
    "            test_error = 0\n",
    "            test_correct_cnt = 0\n",
    "            for index, observation in enumerate(test_images_array):\n",
    "                #forward propagation\n",
    "                layer_0 = test_images_array[index:index+1]\n",
    "                layer_1 = relu(layer_0.dot(weights['weights_0_1']))\n",
    "                layer_2 = layer_1.dot(weights['weights_1_2'])\n",
    "\n",
    "                layer_2_delta = layer_2 - test_labels_array[index:index+1]\n",
    "\n",
    "                test_error += np.sum((layer_2_delta ** 2))\n",
    "                test_correct_cnt += int(np.argmax(layer_2) == np.argmax(test_labels_array[index:index+1]))\n",
    "\n",
    "            print('Train err: ' + str(train_error/len(train_images_array)) + ' Train acc: ' + str(train_correct_cnt) \\\n",
    "                      + ' Test err: ' + str(test_error/len(test_images_array)) + ' Test acc: ' + str(test_correct_cnt))\n",
    "            \n",
    "            \n",
    "neural_network(train_images_array, weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Add better activation functions - Tanh for hidden layer and Softmax for output layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc: 171 Test acc: 99\n",
      "Train acc: 631 Test acc: 569\n",
      "Train acc: 719 Test acc: 617\n",
      "Train acc: 744 Test acc: 670\n",
      "Train acc: 798 Test acc: 717\n",
      "Train acc: 829 Test acc: 735\n",
      "Train acc: 838 Test acc: 756\n",
      "Train acc: 859 Test acc: 770\n",
      "Train acc: 880 Test acc: 788\n",
      "Train acc: 870 Test acc: 793\n",
      "Train acc: 884 Test acc: 805\n",
      "Train acc: 893 Test acc: 809\n",
      "Train acc: 902 Test acc: 812\n",
      "Train acc: 909 Test acc: 814\n",
      "Train acc: 901 Test acc: 818\n",
      "Train acc: 911 Test acc: 818\n",
      "Train acc: 918 Test acc: 822\n",
      "Train acc: 923 Test acc: 825\n",
      "Train acc: 917 Test acc: 827\n",
      "Train acc: 920 Test acc: 824\n",
      "Train acc: 924 Test acc: 832\n",
      "Train acc: 918 Test acc: 831\n",
      "Train acc: 924 Test acc: 840\n",
      "Train acc: 932 Test acc: 841\n",
      "Train acc: 937 Test acc: 832\n",
      "Train acc: 937 Test acc: 839\n",
      "Train acc: 941 Test acc: 842\n",
      "Train acc: 934 Test acc: 839\n",
      "Train acc: 937 Test acc: 837\n",
      "Train acc: 945 Test acc: 844\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "iterations, alpha = 300, 2\n",
    "\n",
    "hidden_size, batch_size = 100, 100\n",
    "\n",
    "weights_0_1 = .02 * np.random.random((train_images_array.shape[1], hidden_size)) - .01\n",
    "weights_1_2 = .2 * np.random.random((hidden_size, train_labels_array.shape[1])) - .1\n",
    "\n",
    "weights = {\n",
    "    'weights_0_1': weights_0_1,\n",
    "    'weights_1_2': weights_1_2\n",
    "}\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh2deriv(output):\n",
    "    return 1 - output ** 2\n",
    "\n",
    "def softmax(x):\n",
    "    tmp = np.exp(x)\n",
    "    return tmp / np.sum(tmp, axis=1, keepdims=True)\n",
    "\n",
    "def neural_network(input: np.ndarray, weights: dict) -> None:\n",
    "    for iteration in range(iterations):\n",
    "        #train_error = 0 no error function for we are not ready for that yet - cross entropy\n",
    "        train_correct_cnt = 0\n",
    "        for index in range(int(len(train_images_array)/batch_size)):\n",
    "            \n",
    "            #batch\n",
    "            batch_start, batch_end = index * batch_size, (index + 1) * batch_size\n",
    "            \n",
    "            #forward propagation\n",
    "            layer_0 = input[batch_start:batch_end]            \n",
    "            \n",
    "            layer_1 = tanh(layer_0.dot(weights['weights_0_1']))\n",
    "            dropout_mask = np.random.randint(2, size=layer_1.shape)\n",
    "            layer_1 *= dropout_mask * 2\n",
    "            \n",
    "            layer_2 = softmax(layer_1.dot(weights['weights_1_2']))\n",
    "            \n",
    "            layer_2_delta = (layer_2 - train_labels_array[batch_start:batch_end]) / (batch_size * layer_2.shape[0])\n",
    "            \n",
    "            train_correct_cnt += np.sum((np.argmax(layer_2, axis=1) == np.argmax(train_labels_array[batch_start:batch_end], axis=1)).astype(int))\n",
    "            \n",
    "            layer_1_delta = layer_2_delta.dot(weights['weights_1_2'].T) \\\n",
    "                                * tanh2deriv(layer_1)\n",
    "            layer_1_delta *= dropout_mask\n",
    "            \n",
    "            weights_1_2_delta = layer_1.T.dot(layer_2_delta)\n",
    "            weights_0_1_delta = layer_0.T.dot(layer_1_delta)\n",
    "            \n",
    "            weights['weights_1_2'] -= weights_1_2_delta * alpha\n",
    "            weights['weights_0_1'] -= weights_0_1_delta * alpha\n",
    "            \n",
    "        \n",
    "        if(iteration % 10 == 0):\n",
    "             #test_error = 0 no error function for we are not ready for that yet - cross entropy\n",
    "            test_correct_cnt = 0\n",
    "            for index, observation in enumerate(test_images_array):\n",
    "                #forward propagation\n",
    "                layer_0 = test_images_array[index:index+1]\n",
    "                layer_1 = tanh(layer_0.dot(weights['weights_0_1']))\n",
    "                layer_2 = layer_1.dot(weights['weights_1_2'])\n",
    "\n",
    "                test_correct_cnt += int(np.argmax(layer_2) == np.argmax(test_labels_array[index:index+1]))\n",
    "\n",
    "            print('Train acc: ' + str(train_correct_cnt) \\\n",
    "                      + ' Test acc: ' + str(test_correct_cnt))\n",
    "            \n",
    "            \n",
    "neural_network(train_images_array, weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Add Convolutional layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 Train acc: 93 Test acc: 92\n",
      "Iteration: 10 Train acc: 191 Test acc: 273\n",
      "Iteration: 20 Train acc: 350 Test acc: 429\n",
      "Iteration: 30 Train acc: 507 Test acc: 584\n",
      "Iteration: 40 Train acc: 609 Test acc: 635\n",
      "Iteration: 50 Train acc: 668 Test acc: 682\n",
      "Iteration: 60 Train acc: 693 Test acc: 697\n",
      "Iteration: 70 Train acc: 712 Test acc: 720\n",
      "Iteration: 80 Train acc: 727 Test acc: 739\n",
      "Iteration: 90 Train acc: 747 Test acc: 745\n",
      "Iteration: 100 Train acc: 768 Test acc: 757\n",
      "Iteration: 110 Train acc: 757 Test acc: 779\n",
      "Iteration: 120 Train acc: 768 Test acc: 785\n",
      "Iteration: 130 Train acc: 775 Test acc: 794\n",
      "Iteration: 140 Train acc: 784 Test acc: 796\n",
      "Iteration: 150 Train acc: 782 Test acc: 797\n",
      "Iteration: 160 Train acc: 791 Test acc: 801\n",
      "Iteration: 170 Train acc: 802 Test acc: 802\n",
      "Iteration: 180 Train acc: 793 Test acc: 808\n",
      "Iteration: 190 Train acc: 809 Test acc: 807\n",
      "Iteration: 200 Train acc: 803 Test acc: 805\n",
      "Iteration: 210 Train acc: 818 Test acc: 811\n",
      "Iteration: 220 Train acc: 814 Test acc: 811\n",
      "Iteration: 230 Train acc: 819 Test acc: 818\n",
      "Iteration: 240 Train acc: 817 Test acc: 819\n",
      "Iteration: 250 Train acc: 833 Test acc: 818\n",
      "Iteration: 260 Train acc: 819 Test acc: 816\n",
      "Iteration: 270 Train acc: 828 Test acc: 818\n",
      "Iteration: 280 Train acc: 834 Test acc: 816\n",
      "Iteration: 290 Train acc: 842 Test acc: 815\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "iterations, alpha = 300, 2\n",
    "\n",
    "batch_size = 128\n",
    "input_rows, input_cols = 28, 28\n",
    "num_kernels, kernel_rows, kernel_cols = 16, 3, 3\n",
    "hidden_size = (input_rows - kernel_rows) * (input_cols - kernel_cols) * num_kernels # 10000\n",
    "\n",
    "kernels = .02 * np.random.random((kernel_rows * kernel_cols, num_kernels)) - .01\n",
    "weights_1_2 = .2 * np.random.random((hidden_size, train_labels_array.shape[1])) - .1\n",
    "\n",
    "weights = {\n",
    "    'kernels': kernels,\n",
    "    'weights_1_2': weights_1_2\n",
    "}\n",
    "\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh2deriv(output):\n",
    "    return 1 - output ** 2\n",
    "\n",
    "def softmax(x):\n",
    "    tmp = np.exp(x)\n",
    "    return tmp / np.sum(tmp, axis=1, keepdims=True)\n",
    "\n",
    "def get_image_section(layer, row_from, row_to, col_from, col_to): \n",
    "    subsection = layer[:, row_from:row_to, col_from:col_to]\n",
    "    return subsection.reshape(-1, 1, row_to-row_from, col_to-col_from)\n",
    "\n",
    "def neural_network(input: np.ndarray, weights: dict) -> None:\n",
    "    for iteration in range(iterations):\n",
    "        #train_error = 0 no error function for we are not ready for that yet - cross entropy\n",
    "        train_correct_cnt = 0\n",
    "        for index in range(int(len(train_images_array)/batch_size)):\n",
    "            \n",
    "            #batch\n",
    "            batch_start, batch_end = index * batch_size, (index + 1) * batch_size\n",
    "            \n",
    "            #forward propagation\n",
    "            layer_0 = input[batch_start:batch_end]            \n",
    "            layer_0 = layer_0.reshape(-1, input_rows, input_cols)\n",
    "            \n",
    "            #convolutional layer\n",
    "            sects = list()\n",
    "            for row_start in range(layer_0.shape[1] - kernel_rows):\n",
    "                for col_start in range(layer_0.shape[2] - kernel_cols):\n",
    "                    sect = get_image_section(layer_0,\n",
    "                                            row_start, row_start+kernel_rows,\n",
    "                                            col_start, col_start+kernel_cols)\n",
    "                    \n",
    "                    sects.append(sect)\n",
    "                    \n",
    "            expanded_input = np.concatenate(sects, axis=1) #(128, 625, 3, 3)           \n",
    "            flattened_input = expanded_input.reshape(expanded_input.shape[0]*expanded_input.shape[1], -1) #(80000, 9)            \n",
    "            kernel_output = flattened_input.dot(weights['kernels']) #(80000, 16)\n",
    "            \n",
    "            layer_1 = tanh(kernel_output.reshape(expanded_input.shape[0], -1)) #(128, 10000)\n",
    "            \n",
    "            dropout_mask = np.random.randint(2, size=layer_1.shape)\n",
    "            layer_1 *= dropout_mask * 2\n",
    "            \n",
    "            layer_2 = softmax(layer_1.dot(weights['weights_1_2']))\n",
    "            \n",
    "            layer_2_delta = (layer_2 - train_labels_array[batch_start:batch_end]) / (batch_size * layer_2.shape[0])\n",
    "            \n",
    "            train_correct_cnt += np.sum((np.argmax(layer_2, axis=1) == np.argmax(train_labels_array[batch_start:batch_end], axis=1)).astype(int))\n",
    "            \n",
    "            layer_1_delta = layer_2_delta.dot(weights['weights_1_2'].T) \\\n",
    "                                * tanh2deriv(layer_1)\n",
    "            layer_1_delta *= dropout_mask\n",
    "            \n",
    "            weights_1_2_delta = layer_1.T.dot(layer_2_delta)\n",
    "            \n",
    "            #Updating kernels\n",
    "            layer_1_delta_reshaped = layer_1_delta.reshape(kernel_output.shape)\n",
    "            kernel_update = flattened_input.T.dot(layer_1_delta_reshaped)\n",
    "            \n",
    "            weights['weights_1_2'] -= weights_1_2_delta * alpha\n",
    "            weights['kernels'] -= kernel_update * alpha\n",
    "            \n",
    "        #continue from here ....\n",
    "        if(iteration % 10 == 0):\n",
    "            #test_error = 0 no error function for we are not ready for that yet - cross entropy\n",
    "            test_correct_cnt = 0\n",
    "            for index, observation in enumerate(test_images_array):\n",
    "                \n",
    "                #forward propagation\n",
    "                layer_0 = test_images_array[index:index+1]\n",
    "                layer_0 = layer_0.reshape(-1, input_rows, input_cols)\n",
    "                \n",
    "                sects = list()\n",
    "                for row_start in range(layer_0.shape[1] - kernel_rows):\n",
    "                    for col_start in range(layer_0.shape[2] - kernel_cols):\n",
    "                        sect = get_image_section(layer_0, \n",
    "                                                 row_start, row_start + kernel_rows,\n",
    "                                                 col_start, col_start + kernel_cols)\n",
    "                        sects.append(sect)\n",
    "                        \n",
    "                expanded_input = np.concatenate(sects, axis=1)\n",
    "                flattened_input = expanded_input.reshape(expanded_input.shape[0]*expanded_input.shape[1], -1)\n",
    "                kernel_output = flattened_input.dot(kernels)\n",
    "                \n",
    "                layer_1 = tanh(kernel_output.reshape(expanded_input.shape[0], -1))\n",
    "                layer_2 = layer_1.dot(weights['weights_1_2'])\n",
    "\n",
    "                test_correct_cnt += int(np.argmax(layer_2) == np.argmax(test_labels_array[index:index+1]))\n",
    "\n",
    "            print('Iteration: ' + str(iteration) + ' Train acc: ' + str(train_correct_cnt) \\\n",
    "                      + ' Test acc: ' + str(test_correct_cnt))\n",
    "            \n",
    "            \n",
    "neural_network(train_images_array, weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One output node trainning => it wouldn't actually work for we would update the same set weights, therefore the last number(image) predicted would set the weights to recognize those specific patterns. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One output node for each of our numbers i.e. 0 to 9;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.random.rand(784, 10);\n",
    "alpha = .00000000001;\n",
    "goal_pred = [];\n",
    "\n",
    "def goal_pred_by_label(label):\n",
    "    return {\n",
    "         0: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "         1: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "         2: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "         3: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
    "         4: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "         5: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
    "         6: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
    "         7: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
    "         8: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
    "         9: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
    "    }.get(label);\n",
    "\n",
    "for i in range(len(imageBs_array)):\n",
    "    goal_pred.append(goal_pred_by_label(labels_array[i]));\n",
    "    \n",
    "def calc_delta(pred, labels_array):\n",
    "    goal_pred, delta = [[], []];\n",
    "    for i in range(len(labels_array)):\n",
    "        goal_pred.append(goal_pred_by_label(labels_array[i]));\n",
    "    for j in range(len(goal_pred)):\n",
    "        delta.append(np.subtract(pred[j], goal_pred[j]));\n",
    "    return delta;\n",
    "        \n",
    "def neural_network(input, weights):\n",
    "    for i in range(1):\n",
    "        pred = np.dot(input, weights);\n",
    "        #print('Weight: ' + str(weights[0]));\n",
    "        msquared_error = (np.subtract(pred, goal_pred)) ** 2;\n",
    "        delta = np.subtract(pred, goal_pred);\n",
    "        weight_delta = np.dot(delta.T, input).T;\n",
    "        #print('Weight_delta: ' + str(weight_delta[0]));\n",
    "        #print('Error: ' + str(msquared_error[0]) + '\\n-----------------------------');\n",
    "        weights = np.subtract(weights, (weight_delta * alpha));\n",
    "        if(i == 99999 or i == 99998 or i == 99997 or i == 99996):\n",
    "            print('Error: ' + str(msquared_error[0]) + '\\n-----------------------------');\n",
    "        #plot_it_all(weights, msquared_error, weight_delta);\n",
    "#         print('Label: ' + str(labels_array[0]) + ' \\nError: ' + str(msquared_error[0])\n",
    "#              + '\\nPred: ' + str(pred[0]) + '\\nGoal_pred: ' + str(goal_pred[0]) \n",
    "#              + '\\n-----------------------------');\n",
    "\n",
    "def plot_it_all(weights, errors, derivatives):\n",
    "    \n",
    "    ax1.set_title('How much changing each weight' \n",
    "                  + '\\n contributed to the error?');\n",
    "    ax1.set_ylabel('Mean squared error');\n",
    "    ax1.set_xlabel('Weight');\n",
    "    ax1.scatter(weights, errors, s=None, c='g');\n",
    "    ax1.plot(weights, errors);\n",
    "    for i in range(len(weights)):\n",
    "        ax1.annotate(i, (weights[i], errors[i]));\n",
    "\n",
    "neural_network(images_array, weights);7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST stochastic gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "I:0 Train Error:22.65 Train Correct:0.299 Test Error: 18.69 Test Correct: 0.489\n",
      "I:10 Train Error:10.98 Train Correct:0.608 Test Error: 9.846 Test Correct: 0.583\n",
      "I:20 Train Error:9.138 Train Correct:0.667 Test Error: 10.34 Test Correct: 0.654\n",
      "I:30 Train Error:9.256 Train Correct:0.677 Test Error: 11.16 Test Correct: 0.6\n",
      "I:40 Train Error:9.656 Train Correct:0.678 Test Error: 7.557 Test Correct: 0.662\n",
      "I:50 Train Error:9.443 Train Correct:0.694 Test Error: 8.033 Test Correct: 0.66\n",
      "I:60 Train Error:9.879 Train Correct:0.688 Test Error: 9.030 Test Correct: 0.698\n",
      "I:70 Train Error:9.843 Train Correct:0.685 Test Error: 8.937 Test Correct: 0.695\n",
      "I:80 Train Error:9.548 Train Correct:0.685 Test Error: 8.801 Test Correct: 0.707\n",
      "I:90 Train Error:8.859 Train Correct:0.698 Test Error: 9.016 Test Correct: 0.712\n",
      "I:100 Train Error:9.345 Train Correct:0.703 Test Error: 8.374 Test Correct: 0.706\n",
      "I:110 Train Error:10.22 Train Correct:0.682 Test Error: 7.809 Test Correct: 0.707\n",
      "I:120 Train Error:10.08 Train Correct:0.691 Test Error: 8.635 Test Correct: 0.683\n",
      "I:130 Train Error:9.092 Train Correct:0.707 Test Error: 7.472 Test Correct: 0.67\n",
      "I:140 Train Error:8.332 Train Correct:0.726 Test Error: 9.047 Test Correct: 0.718\n",
      "I:150 Train Error:8.001 Train Correct:0.722 Test Error: 8.233 Test Correct: 0.711\n",
      "I:160 Train Error:8.743 Train Correct:0.713 Test Error: 8.392 Test Correct: 0.701\n",
      "I:170 Train Error:8.740 Train Correct:0.723 Test Error: 8.514 Test Correct: 0.719\n",
      "I:180 Train Error:8.882 Train Correct:0.723 Test Error: 7.758 Test Correct: 0.718\n",
      "I:190 Train Error:9.232 Train Correct:0.718 Test Error: 8.471 Test Correct: 0.736\n",
      "I:200 Train Error:8.978 Train Correct:0.725 Test Error: 8.758 Test Correct: 0.703\n",
      "I:210 Train Error:8.055 Train Correct:0.734 Test Error: 7.612 Test Correct: 0.722\n",
      "I:220 Train Error:8.542 Train Correct:0.74 Test Error: 8.727 Test Correct: 0.688\n",
      "I:230 Train Error:8.553 Train Correct:0.738 Test Error: 7.899 Test Correct: 0.738\n",
      "I:240 Train Error:7.952 Train Correct:0.746 Test Error: 8.238 Test Correct: 0.733\n",
      "I:250 Train Error:8.036 Train Correct:0.745 Test Error: 8.431 Test Correct: 0.747\n",
      "I:260 Train Error:8.664 Train Correct:0.741 Test Error: 7.907 Test Correct: 0.741\n",
      "I:270 Train Error:8.127 Train Correct:0.74 Test Error: 8.038 Test Correct: 0.744\n",
      "I:280 Train Error:7.772 Train Correct:0.765 Test Error: 7.558 Test Correct: 0.728\n",
      "I:290 Train Error:7.923 Train Correct:0.755 Test Error: 8.404 Test Correct: 0.747\n",
      "I:300 Train Error:8.277 Train Correct:0.747 Test Error: 9.679 Test Correct: 0.76\n",
      "I:310 Train Error:8.546 Train Correct:0.744 Test Error: 9.421 Test Correct: 0.736\n",
      "I:320 Train Error:8.624 Train Correct:0.767 Test Error: 8.438 Test Correct: 0.751\n",
      "I:330 Train Error:7.840 Train Correct:0.78 Test Error: 8.924 Test Correct: 0.743\n",
      "I:340 Train Error:8.086 Train Correct:0.76 Test Error: 8.399 Test Correct: 0.757\n",
      "I:350 Train Error:7.617 Train Correct:0.779 Test Error: 7.178 Test Correct: 0.719\n",
      "I:360 Train Error:8.549 Train Correct:0.763 Test Error: 8.486 Test Correct: 0.755\n",
      "I:370 Train Error:8.416 Train Correct:0.763 Test Error: 7.838 Test Correct: 0.754\n",
      "I:380 Train Error:8.448 Train Correct:0.768 Test Error: 8.541 Test Correct: 0.745\n",
      "I:390 Train Error:8.409 Train Correct:0.754 Test Error: 8.216 Test Correct: 0.749\n",
      "I:400 Train Error:8.852 Train Correct:0.765 Test Error: 9.358 Test Correct: 0.763\n",
      "I:410 Train Error:8.695 Train Correct:0.763 Test Error: 9.264 Test Correct: 0.76\n",
      "I:420 Train Error:8.066 Train Correct:0.786 Test Error: 8.495 Test Correct: 0.76\n",
      "I:430 Train Error:7.975 Train Correct:0.763 Test Error: 8.009 Test Correct: 0.746\n",
      "I:440 Train Error:8.276 Train Correct:0.781 Test Error: 8.605 Test Correct: 0.748\n",
      "I:450 Train Error:9.459 Train Correct:0.76 Test Error: 8.555 Test Correct: 0.745\n",
      "I:460 Train Error:8.461 Train Correct:0.796 Test Error: 8.550 Test Correct: 0.754\n",
      "I:470 Train Error:8.308 Train Correct:0.769 Test Error: 8.161 Test Correct: 0.763\n",
      "I:480 Train Error:9.465 Train Correct:0.741 Test Error: 9.731 Test Correct: 0.763\n",
      "I:490 Train Error:8.867 Train Correct:0.774 Test Error: 8.582 Test Correct: 0.75"
     ]
    }
   ],
   "source": [
    "np.random.seed(1);\n",
    "\n",
    "alpha, iterations, hidden_size = (.001, 500, 400); \n",
    "pixels_per_image, num_of_labels = (784, 10);\n",
    "\n",
    "synapse_0 = .2 * np.random.random((pixels_per_image, hidden_size)) - .1;\n",
    "synapse_1 = .2 * np.random.random((hidden_size, num_of_labels)) - .1;\n",
    "\n",
    "relu = lambda x:(x > 0) * x;\n",
    "relu2deriv = lambda x:(x > 0);\n",
    "\n",
    "for iteration in range(iterations):\n",
    "    msquared_error_layer_2, correct_cnt = (0.0, 0);\n",
    "    for index in range(len(train_images_array)):\n",
    "        #forward propagation\n",
    "        layer_0 = train_images_array[index:index+1];\n",
    "        layer_1 = relu(layer_0.dot(synapse_0));\n",
    "        \n",
    "        #dropout\n",
    "        dropout_mask = np.random.randint(2, size=layer_1.shape);\n",
    "        layer_1 *= dropout_mask * 2;\n",
    "        \n",
    "        layer_2 = layer_1.dot(synapse_1);#l_2 = relu(l_0S_0)S_1;\n",
    "        #print(layer_2);\n",
    "        msquared_error_layer_2 += np.sum((layer_2 \n",
    "                - train_labels_array[index:index+1]) ** 2);\n",
    "        \n",
    "        correct_cnt += int(np.argmax(layer_2) == np.argmax(train_labels_array[index:index+1]));\n",
    "        \n",
    "        layer_2_delta = layer_2 - train_labels_array[index:index+1];\n",
    "        \n",
    "        layer_1_delta = layer_2_delta.dot(synapse_1.T) * relu2deriv(layer_1);\n",
    "        layer_1_delta *= dropout_mask;\n",
    "        \n",
    "        synapse_1_delta = layer_1.T.dot(layer_2_delta);\n",
    "        synapse_0_delta = layer_0.T.dot(layer_1_delta);\n",
    "        \n",
    "        synapse_1 -= synapse_1_delta * alpha;\n",
    "        synapse_0 -= synapse_0_delta * alpha;\n",
    "    #time for inference    \n",
    "    if(iteration % 10 == 0):\n",
    "        #page -r msquared_error_layer_2\n",
    "        #print(msquared_error_layer_2);\n",
    "        #time for inference \n",
    "        msquared_error_test, correct_cnt_test = (0.0, 0.0);\n",
    "        for index in range(len(test_images_array)):\n",
    "            layer_0 = test_images_array[index:index+1];\n",
    "            layer_1 = relu(layer_0.dot(synapse_0));\n",
    "            layer_2 = layer_1.dot(synapse_1);\n",
    "    \n",
    "            msquared_error_test += np.sum((layer_2 - test_labels_array[index:index+1]) ** 2);\n",
    "            correct_cnt_test += int(np.argmax(layer_2) == np.argmax(test_labels_array[index:index+1]));\n",
    "            \n",
    "        sys.stdout.write(\"\\n\"\n",
    "                             + \"I:\" + str(iteration)\n",
    "                             + \" Train Error:\" + str(msquared_error_layer_2/float(len(train_images_array)))[0:5]\n",
    "                             + \" Train Correct:\" + str(correct_cnt/len(train_images_array))\n",
    "                             + \" Test Error: \" + str(msquared_error_test/len(test_images_array))[0:5]\n",
    "                              +\" Test Correct: \" + str(correct_cnt_test/len(test_images_array)));\n",
    "                                 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mini-bitched stochastic gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "I:0 Train Error:68.47 Train Correct:0.159 Test Error: 26.20 Test Correct: 0.249\n",
      "I:10 Train Error:16.62 Train Correct:0.388 Test Error: 17.53 Test Correct: 0.364\n",
      "I:20 Train Error:12.72 Train Correct:0.494 Test Error: 14.17 Test Correct: 0.469\n",
      "I:30 Train Error:11.17 Train Correct:0.532 Test Error: 12.09 Test Correct: 0.521\n",
      "I:40 Train Error:10.53 Train Correct:0.575 Test Error: 11.06 Test Correct: 0.569\n",
      "I:50 Train Error:9.583 Train Correct:0.604 Test Error: 10.07 Test Correct: 0.607\n",
      "I:60 Train Error:9.053 Train Correct:0.642 Test Error: 9.406 Test Correct: 0.624\n",
      "I:70 Train Error:8.277 Train Correct:0.662 Test Error: 9.060 Test Correct: 0.616\n",
      "I:80 Train Error:7.795 Train Correct:0.683 Test Error: 8.583 Test Correct: 0.647\n",
      "I:90 Train Error:7.250 Train Correct:0.703 Test Error: 8.761 Test Correct: 0.654\n",
      "I:100 Train Error:7.290 Train Correct:0.695 Test Error: 8.535 Test Correct: 0.671\n",
      "I:110 Train Error:6.904 Train Correct:0.703 Test Error: 8.314 Test Correct: 0.679\n",
      "I:120 Train Error:6.701 Train Correct:0.716 Test Error: 7.742 Test Correct: 0.664\n",
      "I:130 Train Error:6.576 Train Correct:0.719 Test Error: 8.407 Test Correct: 0.696\n",
      "I:140 Train Error:6.407 Train Correct:0.746 Test Error: 7.920 Test Correct: 0.686\n",
      "I:150 Train Error:6.277 Train Correct:0.731 Test Error: 8.195 Test Correct: 0.695\n",
      "I:160 Train Error:5.675 Train Correct:0.736 Test Error: 7.940 Test Correct: 0.702\n",
      "I:170 Train Error:5.663 Train Correct:0.747 Test Error: 7.808 Test Correct: 0.705\n",
      "I:180 Train Error:5.574 Train Correct:0.757 Test Error: 7.724 Test Correct: 0.701\n",
      "I:190 Train Error:5.683 Train Correct:0.763 Test Error: 7.563 Test Correct: 0.705\n",
      "I:200 Train Error:5.583 Train Correct:0.765 Test Error: 7.176 Test Correct: 0.692\n",
      "I:210 Train Error:5.151 Train Correct:0.776 Test Error: 7.116 Test Correct: 0.703\n",
      "I:220 Train Error:5.371 Train Correct:0.766 Test Error: 7.055 Test Correct: 0.712\n",
      "I:230 Train Error:5.564 Train Correct:0.783 Test Error: 7.112 Test Correct: 0.705\n",
      "I:240 Train Error:4.801 Train Correct:0.777 Test Error: 6.896 Test Correct: 0.69\n",
      "I:250 Train Error:4.995 Train Correct:0.787 Test Error: 6.893 Test Correct: 0.717\n",
      "I:260 Train Error:5.485 Train Correct:0.779 Test Error: 6.774 Test Correct: 0.708\n",
      "I:270 Train Error:4.789 Train Correct:0.782 Test Error: 6.932 Test Correct: 0.709\n",
      "I:280 Train Error:4.888 Train Correct:0.783 Test Error: 6.735 Test Correct: 0.717\n",
      "I:290 Train Error:4.996 Train Correct:0.78 Test Error: 7.400 Test Correct: 0.726\n",
      "I:300 Train Error:4.553 Train Correct:0.797 Test Error: 7.025 Test Correct: 0.72\n",
      "I:310 Train Error:4.570 Train Correct:0.789 Test Error: 6.639 Test Correct: 0.703\n",
      "I:320 Train Error:4.685 Train Correct:0.784 Test Error: 7.081 Test Correct: 0.727\n",
      "I:330 Train Error:4.508 Train Correct:0.792 Test Error: 7.131 Test Correct: 0.721\n",
      "I:340 Train Error:4.361 Train Correct:0.793 Test Error: 7.656 Test Correct: 0.735\n",
      "I:350 Train Error:4.440 Train Correct:0.798 Test Error: 6.951 Test Correct: 0.715\n",
      "I:360 Train Error:4.062 Train Correct:0.8 Test Error: 6.915 Test Correct: 0.724\n",
      "I:370 Train Error:3.826 Train Correct:0.799 Test Error: 7.030 Test Correct: 0.722\n",
      "I:380 Train Error:4.518 Train Correct:0.8 Test Error: 7.060 Test Correct: 0.715\n",
      "I:390 Train Error:4.294 Train Correct:0.798 Test Error: 6.585 Test Correct: 0.728\n",
      "I:400 Train Error:4.090 Train Correct:0.796 Test Error: 6.750 Test Correct: 0.731\n",
      "I:410 Train Error:4.223 Train Correct:0.801 Test Error: 7.135 Test Correct: 0.737\n",
      "I:420 Train Error:4.509 Train Correct:0.808 Test Error: 6.824 Test Correct: 0.732\n",
      "I:430 Train Error:4.179 Train Correct:0.811 Test Error: 6.563 Test Correct: 0.726\n",
      "I:440 Train Error:4.238 Train Correct:0.8 Test Error: 7.237 Test Correct: 0.724\n",
      "I:450 Train Error:4.289 Train Correct:0.817 Test Error: 7.120 Test Correct: 0.732\n",
      "I:460 Train Error:4.081 Train Correct:0.814 Test Error: 7.108 Test Correct: 0.735\n",
      "I:470 Train Error:3.852 Train Correct:0.806 Test Error: 8.528 Test Correct: 0.736\n",
      "I:480 Train Error:3.798 Train Correct:0.811 Test Error: 7.043 Test Correct: 0.73\n",
      "I:490 Train Error:3.974 Train Correct:0.805 Test Error: 7.260 Test Correct: 0.741"
     ]
    }
   ],
   "source": [
    "np.random.seed(1);\n",
    "\n",
    "batch_size = 100; \n",
    "alpha, iterations, hidden_size = (.01, 500, 400); \n",
    "pixels_per_image, num_of_labels = (784, 10);\n",
    "\n",
    "synapse_0 = .2 * np.random.random((pixels_per_image, hidden_size)) - .1;\n",
    "synapse_1 = .2 * np.random.random((hidden_size, num_of_labels)) - .1;\n",
    "\n",
    "relu = lambda x:(x > 0) * x;\n",
    "relu2deriv = lambda x:(x > 0);\n",
    "\n",
    "for iteration in range(iterations):\n",
    "    msquared_error_layer_2, correct_cnt = (0.0, 0);\n",
    "    for index in range(int(len(train_images_array)/batch_size)):\n",
    "        batch_start, batch_end = (index * batch_size, (index + 1) * batch_size);\n",
    "        #forward propagation\n",
    "        layer_0 = train_images_array[batch_start:batch_end];\n",
    "        layer_1 = relu(layer_0.dot(synapse_0));\n",
    "        #dropout\n",
    "        dropout_mask = np.random.randint(2, size=layer_1.shape);\n",
    "        layer_1 *= dropout_mask * 2;\n",
    "        \n",
    "        layer_2 = layer_1.dot(synapse_1);#l_2 = relu(l_0S_0)S_1;\n",
    "        #print(layer_2);100x10\n",
    "        msquared_error_layer_2 += np.sum((layer_2 \n",
    "                - train_labels_array[batch_start:batch_end]) ** 2);\n",
    "        \n",
    "        for index_cnt in range(batch_size):\n",
    "            correct_cnt += int(np.argmax(layer_2[index_cnt:index_cnt + 1]) == \\\n",
    "                                   np.argmax(train_labels_array[batch_start + index_cnt: batch_start + index_cnt + 1]));\n",
    "            \n",
    "        layer_2_delta = (layer_2 - train_labels_array[batch_start:batch_end]) \\\n",
    "                                        / batch_size;\n",
    "        \n",
    "        layer_1_delta = layer_2_delta.dot(synapse_1.T) * relu2deriv(layer_1);\n",
    "        \n",
    "        #I`ve added the '*2' in the backward step for otherwise\n",
    "        #we would be computing a gradient of a different function\n",
    "        #than we`re evaluating.\n",
    "        #Thus, generally, it`s important to account for anything\n",
    "        #we're doing in the forward step\n",
    "        #in the backward step as well.\n",
    "        #layer_1_delta *= dropout_mask * 2;\n",
    "        \n",
    "        synapse_1_delta = layer_1.T.dot(layer_2_delta);\n",
    "        synapse_0_delta = layer_0.T.dot(layer_1_delta);\n",
    "        \n",
    "        synapse_1 -= synapse_1_delta * alpha;\n",
    "        synapse_0 -= synapse_0_delta * alpha;\n",
    "        \n",
    "    #time for inference    \n",
    "    if(iteration % 10 == 0):\n",
    "        #page -r msquared_error_layer_2\n",
    "        #print(msquared_error_layer_2);\n",
    "        #time for inference \n",
    "        msquared_error_test, correct_cnt_test = (0.0, 0.0);\n",
    "        for index in range(len(test_images_array)):\n",
    "            layer_0 = test_images_array[index:index+1];\n",
    "            layer_1 = relu(layer_0.dot(synapse_0));\n",
    "            layer_2 = layer_1.dot(synapse_1);\n",
    "    \n",
    "            msquared_error_test += np.sum((layer_2 - test_labels_array[index:index+1]) ** 2);\n",
    "            correct_cnt_test += int(np.argmax(layer_2) == np.argmax(test_labels_array[index:index+1]));\n",
    "            \n",
    "        sys.stdout.write(\"\\n\"\n",
    "                             + \"I:\" + str(iteration)\n",
    "                             + \" Train Error:\" + str(msquared_error_layer_2/float(len(train_images_array)))[0:5]\n",
    "                             + \" Train Correct:\" + str(correct_cnt/len(train_images_array))\n",
    "                             + \" Test Error: \" + str(msquared_error_test/len(test_images_array))[0:5]\n",
    "                              +\" Test Correct: \" + str(correct_cnt_test/len(test_images_array)));\n",
    "                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4, 5, 18)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array = np.random.randint(3, size=(3, 4, 5, 6))\n",
    "print(len(array))\n",
    "np.concatenate(array, axis=2).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "weights also can be seen as a high dimensional shape.\n",
    "As you train, this shape molds around your data,\n",
    "learning to distinguish one pattern from another.\n",
    "The images in our testing dataset were slightly different \n",
    "than the patterns in our train set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modeling specific types of phenomenon in data. See you there!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
