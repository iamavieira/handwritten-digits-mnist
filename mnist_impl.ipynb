{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipynb_reader\n",
    "import mnist_load as load\n",
    "\n",
    "#defining data \n",
    "train_images_array = load.train_images_array;\n",
    "train_labels_array = load.train_labels_array;\n",
    "test_images_array = load.test_images_array;\n",
    "test_labels_array = load.test_labels_array;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One output node trainning => it wouldn't actually work for we would update the same set weights, therefore the last number(image) predicted would set the weights to recognize those specific patterns. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One output node for each of our numbers i.e. 0 to 9;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights = np.random.rand(784, 10);\n",
    "# alpha = .00000000001;\n",
    "# goal_pred = [];\n",
    "\n",
    "# def goal_pred_by_label(label):\n",
    "#     return {\n",
    "#          0: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "#          1: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "#          2: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "#          3: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
    "#          4: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "#          5: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
    "#          6: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
    "#          7: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
    "#          8: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
    "#          9: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
    "#     }.get(label);\n",
    "\n",
    "# for i in range(len(images_array)):\n",
    "#     goal_pred.append(goal_pred_by_label(labels_array[i]));\n",
    "    \n",
    "# def calc_delta(pred, labels_array):\n",
    "#     goal_pred, delta = [[], []];\n",
    "#     for i in range(len(labels_array)):\n",
    "#         goal_pred.append(goal_pred_by_label(labels_array[i]));\n",
    "#     for j in range(len(goal_pred)):\n",
    "#         delta.append(np.subtract(pred[j], goal_pred[j]));\n",
    "#     return delta;\n",
    "        \n",
    "# def neural_network(input, weights):\n",
    "#     for i in range(1):\n",
    "#         pred = np.dot(input, weights);\n",
    "#         #print('Weight: ' + str(weights[0]));\n",
    "#         msquared_error = (np.subtract(pred, goal_pred)) ** 2;\n",
    "#         delta = np.subtract(pred, goal_pred);\n",
    "#         weight_delta = np.dot(delta.T, input).T;\n",
    "#         #print('Weight_delta: ' + str(weight_delta[0]));\n",
    "#         #print('Error: ' + str(msquared_error[0]) + '\\n-----------------------------');\n",
    "#         weights = np.subtract(weights, (weight_delta * alpha));\n",
    "#         if(i == 99999 or i == 99998 or i == 99997 or i == 99996):\n",
    "#             print('Error: ' + str(msquared_error[0]) + '\\n-----------------------------');\n",
    "#         #plot_it_all(weights, msquared_error, weight_delta);\n",
    "# #         print('Label: ' + str(labels_array[0]) + ' \\nError: ' + str(msquared_error[0])\n",
    "# #              + '\\nPred: ' + str(pred[0]) + '\\nGoal_pred: ' + str(goal_pred[0]) \n",
    "# #              + '\\n-----------------------------');\n",
    "\n",
    "# def plot_it_all(weights, errors, derivatives):\n",
    "    \n",
    "#     ax1.set_title('How much changing each weight' \n",
    "#                   + '\\n contributed to the error?');\n",
    "#     ax1.set_ylabel('Mean squared error');\n",
    "#     ax1.set_xlabel('Weight');\n",
    "#     ax1.scatter(weights, errors, s=None, c='g');\n",
    "#     ax1.plot(weights, errors);\n",
    "#     for i in range(len(weights)):\n",
    "#         ax1.annotate(i, (weights[i], errors[i]));\n",
    "\n",
    "# neural_network(images_array, weights);7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST stochastic gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "I:0 Train Error:22.65 Train Correct:0.299 Test Error: 18.69 Test Correct: 0.489\n",
      "I:10 Train Error:10.98 Train Correct:0.608 Test Error: 9.846 Test Correct: 0.583\n",
      "I:20 Train Error:9.138 Train Correct:0.667 Test Error: 10.34 Test Correct: 0.654\n",
      "I:30 Train Error:9.256 Train Correct:0.677 Test Error: 11.16 Test Correct: 0.6\n",
      "I:40 Train Error:9.656 Train Correct:0.678 Test Error: 7.557 Test Correct: 0.662\n",
      "I:50 Train Error:9.443 Train Correct:0.694 Test Error: 8.033 Test Correct: 0.66\n",
      "I:60 Train Error:9.879 Train Correct:0.688 Test Error: 9.030 Test Correct: 0.698\n",
      "I:70 Train Error:9.843 Train Correct:0.685 Test Error: 8.937 Test Correct: 0.695\n",
      "I:80 Train Error:9.548 Train Correct:0.685 Test Error: 8.801 Test Correct: 0.707\n",
      "I:90 Train Error:8.859 Train Correct:0.698 Test Error: 9.016 Test Correct: 0.712\n",
      "I:100 Train Error:9.345 Train Correct:0.703 Test Error: 8.374 Test Correct: 0.706\n",
      "I:110 Train Error:10.22 Train Correct:0.682 Test Error: 7.809 Test Correct: 0.707\n",
      "I:120 Train Error:10.08 Train Correct:0.691 Test Error: 8.635 Test Correct: 0.683\n",
      "I:130 Train Error:9.092 Train Correct:0.707 Test Error: 7.472 Test Correct: 0.67\n",
      "I:140 Train Error:8.332 Train Correct:0.726 Test Error: 9.047 Test Correct: 0.718\n",
      "I:150 Train Error:8.001 Train Correct:0.722 Test Error: 8.233 Test Correct: 0.711\n",
      "I:160 Train Error:8.743 Train Correct:0.713 Test Error: 8.392 Test Correct: 0.701\n",
      "I:170 Train Error:8.740 Train Correct:0.723 Test Error: 8.514 Test Correct: 0.719\n",
      "I:180 Train Error:8.882 Train Correct:0.723 Test Error: 7.758 Test Correct: 0.718\n",
      "I:190 Train Error:9.232 Train Correct:0.718 Test Error: 8.471 Test Correct: 0.736\n",
      "I:200 Train Error:8.978 Train Correct:0.725 Test Error: 8.758 Test Correct: 0.703\n",
      "I:210 Train Error:8.055 Train Correct:0.734 Test Error: 7.612 Test Correct: 0.722\n",
      "I:220 Train Error:8.542 Train Correct:0.74 Test Error: 8.727 Test Correct: 0.688\n",
      "I:230 Train Error:8.553 Train Correct:0.738 Test Error: 7.899 Test Correct: 0.738\n",
      "I:240 Train Error:7.952 Train Correct:0.746 Test Error: 8.238 Test Correct: 0.733\n",
      "I:250 Train Error:8.036 Train Correct:0.745 Test Error: 8.431 Test Correct: 0.747\n",
      "I:260 Train Error:8.664 Train Correct:0.741 Test Error: 7.907 Test Correct: 0.741\n",
      "I:270 Train Error:8.127 Train Correct:0.74 Test Error: 8.038 Test Correct: 0.744\n",
      "I:280 Train Error:7.772 Train Correct:0.765 Test Error: 7.558 Test Correct: 0.728\n",
      "I:290 Train Error:7.923 Train Correct:0.755 Test Error: 8.404 Test Correct: 0.747\n",
      "I:300 Train Error:8.277 Train Correct:0.747 Test Error: 9.679 Test Correct: 0.76\n",
      "I:310 Train Error:8.546 Train Correct:0.744 Test Error: 9.421 Test Correct: 0.736\n",
      "I:320 Train Error:8.624 Train Correct:0.767 Test Error: 8.438 Test Correct: 0.751\n",
      "I:330 Train Error:7.840 Train Correct:0.78 Test Error: 8.924 Test Correct: 0.743\n",
      "I:340 Train Error:8.086 Train Correct:0.76 Test Error: 8.399 Test Correct: 0.757\n",
      "I:350 Train Error:7.617 Train Correct:0.779 Test Error: 7.178 Test Correct: 0.719\n",
      "I:360 Train Error:8.549 Train Correct:0.763 Test Error: 8.486 Test Correct: 0.755\n",
      "I:370 Train Error:8.416 Train Correct:0.763 Test Error: 7.838 Test Correct: 0.754\n",
      "I:380 Train Error:8.448 Train Correct:0.768 Test Error: 8.541 Test Correct: 0.745\n",
      "I:390 Train Error:8.409 Train Correct:0.754 Test Error: 8.216 Test Correct: 0.749\n",
      "I:400 Train Error:8.852 Train Correct:0.765 Test Error: 9.358 Test Correct: 0.763\n",
      "I:410 Train Error:8.695 Train Correct:0.763 Test Error: 9.264 Test Correct: 0.76\n",
      "I:420 Train Error:8.066 Train Correct:0.786 Test Error: 8.495 Test Correct: 0.76\n",
      "I:430 Train Error:7.975 Train Correct:0.763 Test Error: 8.009 Test Correct: 0.746\n",
      "I:440 Train Error:8.276 Train Correct:0.781 Test Error: 8.605 Test Correct: 0.748\n",
      "I:450 Train Error:9.459 Train Correct:0.76 Test Error: 8.555 Test Correct: 0.745\n",
      "I:460 Train Error:8.461 Train Correct:0.796 Test Error: 8.550 Test Correct: 0.754\n",
      "I:470 Train Error:8.308 Train Correct:0.769 Test Error: 8.161 Test Correct: 0.763\n",
      "I:480 Train Error:9.465 Train Correct:0.741 Test Error: 9.731 Test Correct: 0.763\n",
      "I:490 Train Error:8.867 Train Correct:0.774 Test Error: 8.582 Test Correct: 0.75"
     ]
    }
   ],
   "source": [
    "np.random.seed(1);\n",
    "\n",
    "alpha, iterations, hidden_size = (.001, 500, 400); \n",
    "pixels_per_image, num_of_labels = (784, 10);\n",
    "\n",
    "synapse_0 = .2 * np.random.random((pixels_per_image, hidden_size)) - .1;\n",
    "synapse_1 = .2 * np.random.random((hidden_size, num_of_labels)) - .1;\n",
    "\n",
    "relu = lambda x:(x > 0) * x;\n",
    "relu2deriv = lambda x:(x > 0);\n",
    "\n",
    "for iteration in range(iterations):\n",
    "    msquared_error_layer_2, correct_cnt = (0.0, 0);\n",
    "    for index in range(len(train_images_array)):\n",
    "        #forward propagation\n",
    "        layer_0 = train_images_array[index:index+1];\n",
    "        layer_1 = relu(layer_0.dot(synapse_0));\n",
    "        #dropout\n",
    "        dropout_mask = np.random.randint(2, size=layer_1.shape);\n",
    "        layer_1 *= dropout_mask * 2;\n",
    "        \n",
    "        layer_2 = layer_1.dot(synapse_1);#l_2 = relu(l_0S_0)S_1;\n",
    "        #print(layer_2);\n",
    "        msquared_error_layer_2 += np.sum((layer_2 \n",
    "                - train_labels_array[index:index+1]) ** 2);\n",
    "        \n",
    "        correct_cnt += int(np.argmax(layer_2) == np.argmax(train_labels_array[index:index+1]));\n",
    "        \n",
    "        layer_2_delta = layer_2 - train_labels_array[index:index+1];\n",
    "        \n",
    "        layer_1_delta = layer_2_delta.dot(synapse_1.T) * relu2deriv(layer_1);\n",
    "        layer_1_delta *= dropout_mask;\n",
    "        \n",
    "        synapse_1_delta = layer_1.T.dot(layer_2_delta);\n",
    "        synapse_0_delta = layer_0.T.dot(layer_1_delta);\n",
    "        \n",
    "        synapse_1 -= synapse_1_delta * alpha;\n",
    "        synapse_0 -= synapse_0_delta * alpha;\n",
    "    #time for inference    \n",
    "    if(iteration % 10 == 0):\n",
    "        #page -r msquared_error_layer_2\n",
    "        #print(msquared_error_layer_2);\n",
    "        #time for inference \n",
    "        msquared_error_test, correct_cnt_test = (0.0, 0.0);\n",
    "        for index in range(len(test_images_array)):\n",
    "            layer_0 = test_images_array[index:index+1];\n",
    "            layer_1 = relu(layer_0.dot(synapse_0));\n",
    "            layer_2 = layer_1.dot(synapse_1);\n",
    "    \n",
    "            msquared_error_test += np.sum((layer_2 - test_labels_array[index:index+1]) ** 2);\n",
    "            correct_cnt_test += int(np.argmax(layer_2) == np.argmax(test_labels_array[index:index+1]));\n",
    "            \n",
    "        sys.stdout.write(\"\\n\"\n",
    "                             + \"I:\" + str(iteration)\n",
    "                             + \" Train Error:\" + str(msquared_error_layer_2/float(len(train_images_array)))[0:5]\n",
    "                             + \" Train Correct:\" + str(correct_cnt/len(train_images_array))\n",
    "                             + \" Test Error: \" + str(msquared_error_test/len(test_images_array))[0:5]\n",
    "                              +\" Test Correct: \" + str(correct_cnt_test/len(test_images_array)));\n",
    "                                 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mini-bitched stochastic gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "I:0 Train Error:68.47 Train Correct:0.159 Test Error: 26.20 Test Correct: 0.249\n",
      "I:10 Train Error:16.62 Train Correct:0.388 Test Error: 17.53 Test Correct: 0.364\n",
      "I:20 Train Error:12.72 Train Correct:0.494 Test Error: 14.17 Test Correct: 0.469\n",
      "I:30 Train Error:11.17 Train Correct:0.532 Test Error: 12.09 Test Correct: 0.521\n",
      "I:40 Train Error:10.53 Train Correct:0.575 Test Error: 11.06 Test Correct: 0.569\n",
      "I:50 Train Error:9.583 Train Correct:0.604 Test Error: 10.07 Test Correct: 0.607\n",
      "I:60 Train Error:9.053 Train Correct:0.642 Test Error: 9.406 Test Correct: 0.624\n",
      "I:70 Train Error:8.277 Train Correct:0.662 Test Error: 9.060 Test Correct: 0.616\n",
      "I:80 Train Error:7.795 Train Correct:0.683 Test Error: 8.583 Test Correct: 0.647\n",
      "I:90 Train Error:7.250 Train Correct:0.703 Test Error: 8.761 Test Correct: 0.654\n",
      "I:100 Train Error:7.290 Train Correct:0.695 Test Error: 8.535 Test Correct: 0.671\n",
      "I:110 Train Error:6.904 Train Correct:0.703 Test Error: 8.314 Test Correct: 0.679\n",
      "I:120 Train Error:6.701 Train Correct:0.716 Test Error: 7.742 Test Correct: 0.664\n",
      "I:130 Train Error:6.576 Train Correct:0.719 Test Error: 8.407 Test Correct: 0.696\n",
      "I:140 Train Error:6.407 Train Correct:0.746 Test Error: 7.920 Test Correct: 0.686\n",
      "I:150 Train Error:6.277 Train Correct:0.731 Test Error: 8.195 Test Correct: 0.695\n",
      "I:160 Train Error:5.675 Train Correct:0.736 Test Error: 7.940 Test Correct: 0.702\n",
      "I:170 Train Error:5.663 Train Correct:0.747 Test Error: 7.808 Test Correct: 0.705\n",
      "I:180 Train Error:5.574 Train Correct:0.757 Test Error: 7.724 Test Correct: 0.701\n",
      "I:190 Train Error:5.683 Train Correct:0.763 Test Error: 7.563 Test Correct: 0.705\n",
      "I:200 Train Error:5.583 Train Correct:0.765 Test Error: 7.176 Test Correct: 0.692\n",
      "I:210 Train Error:5.151 Train Correct:0.776 Test Error: 7.116 Test Correct: 0.703\n",
      "I:220 Train Error:5.371 Train Correct:0.766 Test Error: 7.055 Test Correct: 0.712\n",
      "I:230 Train Error:5.564 Train Correct:0.783 Test Error: 7.112 Test Correct: 0.705\n",
      "I:240 Train Error:4.801 Train Correct:0.777 Test Error: 6.896 Test Correct: 0.69\n",
      "I:250 Train Error:4.995 Train Correct:0.787 Test Error: 6.893 Test Correct: 0.717\n",
      "I:260 Train Error:5.485 Train Correct:0.779 Test Error: 6.774 Test Correct: 0.708\n",
      "I:270 Train Error:4.789 Train Correct:0.782 Test Error: 6.932 Test Correct: 0.709\n",
      "I:280 Train Error:4.888 Train Correct:0.783 Test Error: 6.735 Test Correct: 0.717\n",
      "I:290 Train Error:4.996 Train Correct:0.78 Test Error: 7.400 Test Correct: 0.726\n",
      "I:300 Train Error:4.553 Train Correct:0.797 Test Error: 7.025 Test Correct: 0.72\n",
      "I:310 Train Error:4.570 Train Correct:0.789 Test Error: 6.639 Test Correct: 0.703\n",
      "I:320 Train Error:4.685 Train Correct:0.784 Test Error: 7.081 Test Correct: 0.727\n",
      "I:330 Train Error:4.508 Train Correct:0.792 Test Error: 7.131 Test Correct: 0.721\n",
      "I:340 Train Error:4.361 Train Correct:0.793 Test Error: 7.656 Test Correct: 0.735\n",
      "I:350 Train Error:4.440 Train Correct:0.798 Test Error: 6.951 Test Correct: 0.715\n",
      "I:360 Train Error:4.062 Train Correct:0.8 Test Error: 6.915 Test Correct: 0.724\n",
      "I:370 Train Error:3.826 Train Correct:0.799 Test Error: 7.030 Test Correct: 0.722\n",
      "I:380 Train Error:4.518 Train Correct:0.8 Test Error: 7.060 Test Correct: 0.715\n",
      "I:390 Train Error:4.294 Train Correct:0.798 Test Error: 6.585 Test Correct: 0.728\n",
      "I:400 Train Error:4.090 Train Correct:0.796 Test Error: 6.750 Test Correct: 0.731\n",
      "I:410 Train Error:4.223 Train Correct:0.801 Test Error: 7.135 Test Correct: 0.737\n",
      "I:420 Train Error:4.509 Train Correct:0.808 Test Error: 6.824 Test Correct: 0.732\n",
      "I:430 Train Error:4.179 Train Correct:0.811 Test Error: 6.563 Test Correct: 0.726\n",
      "I:440 Train Error:4.238 Train Correct:0.8 Test Error: 7.237 Test Correct: 0.724\n",
      "I:450 Train Error:4.289 Train Correct:0.817 Test Error: 7.120 Test Correct: 0.732\n",
      "I:460 Train Error:4.081 Train Correct:0.814 Test Error: 7.108 Test Correct: 0.735\n",
      "I:470 Train Error:3.852 Train Correct:0.806 Test Error: 8.528 Test Correct: 0.736\n",
      "I:480 Train Error:3.798 Train Correct:0.811 Test Error: 7.043 Test Correct: 0.73\n",
      "I:490 Train Error:3.974 Train Correct:0.805 Test Error: 7.260 Test Correct: 0.741"
     ]
    }
   ],
   "source": [
    "np.random.seed(1);\n",
    "\n",
    "batch_size = 100; \n",
    "alpha, iterations, hidden_size = (.01, 500, 400); \n",
    "pixels_per_image, num_of_labels = (784, 10);\n",
    "\n",
    "synapse_0 = .2 * np.random.random((pixels_per_image, hidden_size)) - .1;\n",
    "synapse_1 = .2 * np.random.random((hidden_size, num_of_labels)) - .1;\n",
    "\n",
    "relu = lambda x:(x > 0) * x;\n",
    "relu2deriv = lambda x:(x > 0);\n",
    "\n",
    "for iteration in range(iterations):\n",
    "    msquared_error_layer_2, correct_cnt = (0.0, 0);\n",
    "    for index in range(int(len(train_images_array)/batch_size)):\n",
    "        batch_start, batch_end = (index * batch_size, (index + 1) * batch_size);\n",
    "        #forward propagation\n",
    "        layer_0 = train_images_array[batch_start:batch_end];\n",
    "        layer_1 = relu(layer_0.dot(synapse_0));\n",
    "        #dropout\n",
    "        dropout_mask = np.random.randint(2, size=layer_1.shape);\n",
    "        layer_1 *= dropout_mask * 2;\n",
    "        \n",
    "        layer_2 = layer_1.dot(synapse_1);#l_2 = relu(l_0S_0)S_1;\n",
    "        #print(layer_2);100x10\n",
    "        msquared_error_layer_2 += np.sum((layer_2 \n",
    "                - train_labels_array[batch_start:batch_end]) ** 2);\n",
    "        \n",
    "        for index_cnt in range(batch_size):\n",
    "            correct_cnt += int(np.argmax(layer_2[index_cnt:index_cnt + 1]) == \\\n",
    "                                   np.argmax(train_labels_array[batch_start + index_cnt: batch_start + index_cnt + 1]));\n",
    "            \n",
    "        layer_2_delta = (layer_2 - train_labels_array[batch_start:batch_end]) \\\n",
    "                                        / batch_size;\n",
    "        \n",
    "        layer_1_delta = layer_2_delta.dot(synapse_1.T) * relu2deriv(layer_1);\n",
    "        #I`ve added the '*2' in the backward step for otherwise\n",
    "        #we would be computing a gradient of a different function\n",
    "        #than we`re evaluating.\n",
    "        #Thus, generally, it`s important to account for anything\n",
    "        #we're doing in the forward step\n",
    "        #in the backward step as well.\n",
    "        #layer_1_delta *= dropout_mask * 2;\n",
    "        \n",
    "        synapse_1_delta = layer_1.T.dot(layer_2_delta);\n",
    "        synapse_0_delta = layer_0.T.dot(layer_1_delta);\n",
    "        \n",
    "        synapse_1 -= synapse_1_delta * alpha;\n",
    "        synapse_0 -= synapse_0_delta * alpha;\n",
    "        \n",
    "    #time for inference    \n",
    "    if(iteration % 10 == 0):\n",
    "        #page -r msquared_error_layer_2\n",
    "        #print(msquared_error_layer_2);\n",
    "        #time for inference \n",
    "        msquared_error_test, correct_cnt_test = (0.0, 0.0);\n",
    "        for index in range(len(test_images_array)):\n",
    "            layer_0 = test_images_array[index:index+1];\n",
    "            layer_1 = relu(layer_0.dot(synapse_0));\n",
    "            layer_2 = layer_1.dot(synapse_1);\n",
    "    \n",
    "            msquared_error_test += np.sum((layer_2 - test_labels_array[index:index+1]) ** 2);\n",
    "            correct_cnt_test += int(np.argmax(layer_2) == np.argmax(test_labels_array[index:index+1]));\n",
    "            \n",
    "        sys.stdout.write(\"\\n\"\n",
    "                             + \"I:\" + str(iteration)\n",
    "                             + \" Train Error:\" + str(msquared_error_layer_2/float(len(train_images_array)))[0:5]\n",
    "                             + \" Train Correct:\" + str(correct_cnt/len(train_images_array))\n",
    "                             + \" Test Error: \" + str(msquared_error_test/len(test_images_array))[0:5]\n",
    "                              +\" Test Correct: \" + str(correct_cnt_test/len(test_images_array)));\n",
    "                                 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "weights also can be seen as a high dimensional shape.\n",
    "As you train, this shape molds around your data,\n",
    "learning to distinguish one pattern from another.\n",
    "The images in our testing dataset were slightly different \n",
    "than the patterns in our train set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modeling specific types of phenomenon in data. See you there!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
