{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We adjust the number of input nodes to reflect the number of datapoints in each trainning example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct as st, numpy as np, matplotlib.pyplot as plt\n",
    "import sys\n",
    "# from IPython.core import page\n",
    "# page.page(variable)\n",
    "# %page -r <variablename>\n",
    "\n",
    "filename_train = {'images': 'mnist/train/train-images-idx3-ubyte',\n",
    "                  'labels': 'mnist/train/train-labels-idx1-ubyte'};\n",
    "filename_test = {'images': 'mnist/test/test-images-idx3-ubyte',\n",
    "                 'labels': 'mnist/test/test-labels-idx1-ubyte'};"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_train_set_images(filename): \n",
    "    images_file = open(filename, 'rb');\n",
    "    images_file.seek(0);\n",
    "    magic = st.unpack('>4B', images_file.read(4));\n",
    "    print(magic);\n",
    "    number_images = st.unpack('>I', images_file.read(4))[0];\n",
    "    print('images_n:' + str(number_images));\n",
    "    number_rows = st.unpack('>I', images_file.read(4))[0];\n",
    "    number_columns = st.unpack('>I', images_file.read(4))[0];\n",
    "    print('Rows: ' + str(number_rows) + 'Columns: ' + str(number_columns));\n",
    "    n_bytes =  1000 * (number_rows * number_columns);\n",
    "    train_array_images = 255 - np.asarray(st.unpack('>' + 'B' * n_bytes, \n",
    "                                    images_file.read(n_bytes))).reshape((1000, number_rows, number_columns));\n",
    "    print('Images: ' + str(train_array_images.shape));\n",
    "    return train_array_images;\n",
    "\n",
    "def read_train_set_labels(filename):\n",
    "    labels_file = open(filename, 'rb');\n",
    "    labels_file.seek(0);\n",
    "    magic_number = st.unpack('>4B', labels_file.read(4));\n",
    "    number_items = st.unpack('>I', labels_file.read(4))[0];\n",
    "    n_bytes = 1000;\n",
    "    train_array_labels =  9 - np.asarray(st.unpack('>' + 'B' * n_bytes,\n",
    "                                            labels_file.read(n_bytes)))\n",
    "    print('Labels: ' + str(train_array_labels.shape));\n",
    "    return train_array_labels;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_test_set_images(filename):\n",
    "    images_file = open(filename, 'rb');\n",
    "    images_file.seek(0);\n",
    "    magic = st.unpack('>4B', images_file.read(4));\n",
    "    print(magic);\n",
    "    number_images = st.unpack('>I', images_file.read(4))[0];\n",
    "    number_rows = st.unpack('>I', images_file.read(4))[0];\n",
    "    number_columns = st.unpack('>I', images_file.read(4))[0];\n",
    "    print('Rows: ' + str(number_rows) + 'Columns: ' + str(number_columns));\n",
    "    n_bytes =  1000 * (number_rows * number_columns);\n",
    "    test_array_images = 255 - np.asarray(st.unpack('>' + 'B' * n_bytes, \n",
    "                                    images_file.read(n_bytes))).reshape((1000, number_rows, number_columns));\n",
    "    print('Images: ' + str(test_array_images.shape));\n",
    "    return test_array_images;\n",
    "\n",
    "def read_test_set_labels(filename):\n",
    "    labels_file = open(filename, 'rb');\n",
    "    labels_file.seek(0);\n",
    "    magic_number = st.unpack('>4B', labels_file.read(4));\n",
    "    number_items = st.unpack('>I', labels_file.read(4))[0];\n",
    "    n_bytes = 1000;\n",
    "    test_array_labels =  9 - np.asarray(st.unpack('>' + 'B' * n_bytes,\n",
    "                                            labels_file.read(n_bytes)))\n",
    "    print('Labels: ' + str(test_array_labels.shape));\n",
    "    return test_array_labels;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0, 8, 3)\n",
      "images_n:60000\n",
      "Rows: 28Columns: 28\n",
      "Images: (1000, 28, 28)\n",
      "Labels: (1000,)\n",
      "(1000, 10)\n",
      "(0, 0, 8, 3)\n",
      "Rows: 28Columns: 28\n",
      "Images: (1000, 28, 28)\n",
      "Labels: (1000,)\n",
      "(1000, 10)\n"
     ]
    }
   ],
   "source": [
    "#train\n",
    "#defining train images array\n",
    "train_images_array = read_train_set_images(filename_train['images']) / 255;\n",
    "\n",
    "#defining train labels array\n",
    "train_labels_array = read_train_set_labels(filename_train['labels']);\n",
    "one_hot_train_labels = np.zeros((len(train_labels_array), 10));\n",
    "for idx,val in enumerate(train_labels_array):\n",
    "    one_hot_train_labels[idx][val] = val\n",
    "train_labels_array = one_hot_train_labels;\n",
    "print(train_labels_array.shape);\n",
    "\n",
    "#test\n",
    "#defining test images array\n",
    "test_images_array = read_test_set_images(filename_test['images']) / 255;\n",
    "\n",
    "#defining train labels array\n",
    "test_labels_array = read_test_set_labels(filename_test['labels']);\n",
    "one_hot_test_labels = np.zeros((len(test_labels_array), 10));\n",
    "for idx,val in enumerate(test_labels_array):\n",
    "    one_hot_test_labels[idx][val] = val\n",
    "test_labels_array = one_hot_test_labels;\n",
    "print(test_labels_array.shape);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4,)\n"
     ]
    }
   ],
   "source": [
    "#flatten example\n",
    "zeros = np.zeros((4, 5, 6, 7, 8));\n",
    "zeros = zeros.reshape((*zeros.shape[:1], -1, *zeros.shape[-1:]));\n",
    "print(zeros.shape[:1]);\n",
    "# np.reshape(()) can take a -1 as an argument, meaning 'total array\n",
    "# size divided by product of all other listed dimensions'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 784)\n",
      "(1000, 784)\n"
     ]
    }
   ],
   "source": [
    "#flatten train images array;\n",
    "train_images_array = train_images_array.reshape((*train_images_array.shape[:1], -1));\n",
    "print(train_images_array.shape);\n",
    "\n",
    "#flatten test images array;\n",
    "test_images_array = test_images_array.reshape((*test_images_array.shape[:1], -1));\n",
    "print(test_images_array.shape);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One output node trainning => it wouldn't actually work for we would update the same set weights, therefore the last number(image) predicted would set the weights to recognize those specific patterns. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One output node for each of our numbers i.e. 0 to 9;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights = np.random.rand(784, 10);\n",
    "# alpha = .00000000001;\n",
    "# goal_pred = [];\n",
    "\n",
    "# def goal_pred_by_label(label):\n",
    "#     return {\n",
    "#          0: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "#          1: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "#          2: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "#          3: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
    "#          4: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "#          5: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
    "#          6: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
    "#          7: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
    "#          8: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
    "#          9: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
    "#     }.get(label);\n",
    "\n",
    "# for i in range(len(images_array)):\n",
    "#     goal_pred.append(goal_pred_by_label(labels_array[i]));\n",
    "    \n",
    "# def calc_delta(pred, labels_array):\n",
    "#     goal_pred, delta = [[], []];\n",
    "#     for i in range(len(labels_array)):\n",
    "#         goal_pred.append(goal_pred_by_label(labels_array[i]));\n",
    "#     for j in range(len(goal_pred)):\n",
    "#         delta.append(np.subtract(pred[j], goal_pred[j]));\n",
    "#     return delta;\n",
    "        \n",
    "# def neural_network(input, weights):\n",
    "#     for i in range(1):\n",
    "#         pred = np.dot(input, weights);\n",
    "#         #print('Weight: ' + str(weights[0]));\n",
    "#         msquared_error = (np.subtract(pred, goal_pred)) ** 2;\n",
    "#         delta = np.subtract(pred, goal_pred);\n",
    "#         weight_delta = np.dot(delta.T, input).T;\n",
    "#         #print('Weight_delta: ' + str(weight_delta[0]));\n",
    "#         #print('Error: ' + str(msquared_error[0]) + '\\n-----------------------------');\n",
    "#         weights = np.subtract(weights, (weight_delta * alpha));\n",
    "#         if(i == 99999 or i == 99998 or i == 99997 or i == 99996):\n",
    "#             print('Error: ' + str(msquared_error[0]) + '\\n-----------------------------');\n",
    "#         #plot_it_all(weights, msquared_error, weight_delta);\n",
    "# #         print('Label: ' + str(labels_array[0]) + ' \\nError: ' + str(msquared_error[0])\n",
    "# #              + '\\nPred: ' + str(pred[0]) + '\\nGoal_pred: ' + str(goal_pred[0]) \n",
    "# #              + '\\n-----------------------------');\n",
    "\n",
    "# def plot_it_all(weights, errors, derivatives):\n",
    "    \n",
    "#     ax1.set_title('How much changing each weight' \n",
    "#                   + '\\n contributed to the error?');\n",
    "#     ax1.set_ylabel('Mean squared error');\n",
    "#     ax1.set_xlabel('Weight');\n",
    "#     ax1.scatter(weights, errors, s=None, c='g');\n",
    "#     ax1.plot(weights, errors);\n",
    "#     for i in range(len(weights)):\n",
    "#         ax1.annotate(i, (weights[i], errors[i]));\n",
    "\n",
    "# neural_network(images_array, weights);7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MNIST stochastic gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(1);\n",
    "\n",
    "alpha, iterations, hidden_size = (.001, 500, 400); \n",
    "pixels_per_image, num_of_labels = (784, 10);\n",
    "\n",
    "synapse_0 = .2 * np.random.random((pixels_per_image, hidden_size)) - .1;\n",
    "synapse_1 = .2 * np.random.random((hidden_size, num_of_labels)) - .1;\n",
    "\n",
    "relu = lambda x:(x > 0) * x;\n",
    "relu2deriv = lambda x:(x > 0);\n",
    "\n",
    "for iteration in range(iterations):\n",
    "    msquared_error_layer_2, correct_cnt = (0.0, 0);\n",
    "    for index in range(len(train_images_array)):\n",
    "        #forward propagation\n",
    "        layer_0 = train_images_array[index:index+1];\n",
    "        layer_1 = relu(layer_0.dot(synapse_0));\n",
    "        #dropout\n",
    "        dropout_mask = np.random.randint(2, size=layer_1.shape);\n",
    "        layer_1 *= dropout_mask * 2;\n",
    "        \n",
    "        layer_2 = layer_1.dot(synapse_1);#l_2 = relu(l_0S_0)S_1;\n",
    "        #print(layer_2);\n",
    "        msquared_error_layer_2 += np.sum((layer_2 \n",
    "                - train_labels_array[index:index+1]) ** 2);\n",
    "        \n",
    "        correct_cnt += int(np.argmax(layer_2) == np.argmax(train_labels_array[index:index+1]));\n",
    "        \n",
    "        layer_2_delta = layer_2 - train_labels_array[index:index+1];\n",
    "        \n",
    "        layer_1_delta = layer_2_delta.dot(synapse_1.T) * relu2deriv(layer_1);\n",
    "        layer_1_delta *= dropout_mask;\n",
    "        \n",
    "        synapse_1_delta = layer_1.T.dot(layer_2_delta);\n",
    "        synapse_0_delta = layer_0.T.dot(layer_1_delta);\n",
    "        \n",
    "        synapse_1 -= synapse_1_delta * alpha;\n",
    "        synapse_0 -= synapse_0_delta * alpha;\n",
    "    #time for inference    \n",
    "    if(iteration % 10 == 0):\n",
    "        #page -r msquared_error_layer_2\n",
    "        #print(msquared_error_layer_2);\n",
    "        #time for inference \n",
    "        msquared_error_test, correct_cnt_test = (0.0, 0.0);\n",
    "        for index in range(len(test_images_array)):\n",
    "            layer_0 = test_images_array[index:index+1];\n",
    "            layer_1 = relu(layer_0.dot(synapse_0));\n",
    "            layer_2 = layer_1.dot(synapse_1);\n",
    "    \n",
    "            msquared_error_test += np.sum((layer_2 - test_labels_array[index:index+1]) ** 2);\n",
    "            correct_cnt_test += int(np.argmax(layer_2) == np.argmax(test_labels_array[index:index+1]));\n",
    "            \n",
    "        sys.stdout.write(\"\\n\"\n",
    "                             + \"I:\" + str(iteration)\n",
    "                             + \" Train Error:\" + str(msquared_error_layer_2/float(len(train_images_array)))[0:5]\n",
    "                             + \" Train Correct:\" + str(correct_cnt/len(train_images_array))\n",
    "                             + \" Test Error: \" + str(msquared_error_test/len(test_images_array))[0:5]\n",
    "                              +\" Test Correct: \" + str(correct_cnt_test/len(test_images_array)));\n",
    "                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini-bitched stochastic gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights also can be seen as a high dimensional shape.\n",
    "As you train, this shape molds around your data,\n",
    "learning to distinguish one pattern from another.\n",
    "The images in our testing dataset were slightly different \n",
    "than the patterns in our train set.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
