{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We adjust the number of input nodes to reflect the number of datapoints in each trainning example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct as st, numpy as np, matplotlib.pyplot as plt\n",
    "import sys\n",
    "# from IPython.core import page\n",
    "# page.page(variable)\n",
    "# %page -r <variablename>\n",
    "\n",
    "filename_train = {'images': 'mnist/train/train-images-idx3-ubyte',\n",
    "                  'labels': 'mnist/train/train-labels-idx1-ubyte'};\n",
    "filename_test = {'images': 'mnist/test/test-images-idx3-ubyte',\n",
    "                 'labels': 'mnist/test/test-labels-idx1-ubyte'};"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_train_set_images(filename): \n",
    "    images_file = open(filename, 'rb');\n",
    "    images_file.seek(0);\n",
    "    magic = st.unpack('>4B', images_file.read(4));\n",
    "    print(magic);\n",
    "    number_images = st.unpack('>I', images_file.read(4))[0];\n",
    "    print('images_n:' + str(number_images));\n",
    "    number_rows = st.unpack('>I', images_file.read(4))[0];\n",
    "    number_columns = st.unpack('>I', images_file.read(4))[0];\n",
    "    print('Rows: ' + str(number_rows) + 'Columns: ' + str(number_columns));\n",
    "    n_bytes =  1000 * (number_rows * number_columns);\n",
    "    train_array_images = 255 - np.asarray(st.unpack('>' + 'B' * n_bytes, \n",
    "                                    images_file.read(n_bytes))).reshape((1000, number_rows, number_columns));\n",
    "    print('Images: ' + str(train_array_images.shape));\n",
    "    return train_array_images;\n",
    "\n",
    "def read_train_set_labels(filename):\n",
    "    labels_file = open(filename, 'rb');\n",
    "    labels_file.seek(0);\n",
    "    magic_number = st.unpack('>4B', labels_file.read(4));\n",
    "    number_items = st.unpack('>I', labels_file.read(4))[0];\n",
    "    n_bytes = 1000;\n",
    "    train_array_labels =  9 - np.asarray(st.unpack('>' + 'B' * n_bytes,\n",
    "                                            labels_file.read(n_bytes)))\n",
    "    print('Labels: ' + str(train_array_labels.shape));\n",
    "    return train_array_labels;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_test_set_images(filename):\n",
    "    images_file = open(filename, 'rb');\n",
    "    images_file.seek(0);\n",
    "    magic = st.unpack('>4B', images_file.read(4));\n",
    "    print(magic);\n",
    "    number_images = st.unpack('>I', images_file.read(4))[0];\n",
    "    number_rows = st.unpack('>I', images_file.read(4))[0];\n",
    "    number_columns = st.unpack('>I', images_file.read(4))[0];\n",
    "    print('Rows: ' + str(number_rows) + 'Columns: ' + str(number_columns));\n",
    "    n_bytes =  1000 * (number_rows * number_columns);\n",
    "    test_array_images = 255 - np.asarray(st.unpack('>' + 'B' * n_bytes, \n",
    "                                    images_file.read(n_bytes))).reshape((1000, number_rows, number_columns));\n",
    "    print('Images: ' + str(test_array_images.shape));\n",
    "    return test_array_images;\n",
    "\n",
    "def read_test_set_labels(filename):\n",
    "    labels_file = open(filename, 'rb');\n",
    "    labels_file.seek(0);\n",
    "    magic_number = st.unpack('>4B', labels_file.read(4));\n",
    "    number_items = st.unpack('>I', labels_file.read(4))[0];\n",
    "    n_bytes = 1000;\n",
    "    test_array_labels =  9 - np.asarray(st.unpack('>' + 'B' * n_bytes,\n",
    "                                            labels_file.read(n_bytes)))\n",
    "    print('Labels: ' + str(test_array_labels.shape));\n",
    "    return test_array_labels;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0, 8, 3)\n",
      "images_n:60000\n",
      "Rows: 28Columns: 28\n",
      "Images: (1000, 28, 28)\n",
      "Labels: (1000,)\n",
      "(1000, 10)\n",
      "(0, 0, 8, 3)\n",
      "Rows: 28Columns: 28\n",
      "Images: (1000, 28, 28)\n",
      "Labels: (1000,)\n",
      "(1000, 10)\n"
     ]
    }
   ],
   "source": [
    "#train\n",
    "#defining train images array\n",
    "train_images_array = read_train_set_images(filename_train['images']) / 255;\n",
    "\n",
    "#defining train labels array\n",
    "train_labels_array = read_train_set_labels(filename_train['labels']);\n",
    "one_hot_train_labels = np.zeros((len(train_labels_array), 10));\n",
    "for idx,val in enumerate(train_labels_array):\n",
    "    one_hot_train_labels[idx][val] = val\n",
    "train_labels_array = one_hot_train_labels;\n",
    "print(train_labels_array.shape);\n",
    "\n",
    "#test\n",
    "#defining test images array\n",
    "test_images_array = read_test_set_images(filename_test['images']) / 255;\n",
    "\n",
    "#defining train labels array\n",
    "test_labels_array = read_test_set_labels(filename_test['labels']);\n",
    "one_hot_test_labels = np.zeros((len(test_labels_array), 10));\n",
    "for idx,val in enumerate(test_labels_array):\n",
    "    one_hot_test_labels[idx][val] = val\n",
    "test_labels_array = one_hot_test_labels;\n",
    "print(test_labels_array.shape);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4,)\n"
     ]
    }
   ],
   "source": [
    "#flatten example\n",
    "zeros = np.zeros((4, 5, 6, 7, 8));\n",
    "zeros = zeros.reshape((*zeros.shape[:1], -1, *zeros.shape[-1:]));\n",
    "print(zeros.shape[:1]);\n",
    "# np.reshape(()) can take a -1 as an argument, meaning 'total array\n",
    "# size divided by product of all other listed dimensions'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 784)\n",
      "(1000, 784)\n"
     ]
    }
   ],
   "source": [
    "#flatten train images array;\n",
    "train_images_array = train_images_array.reshape((*train_images_array.shape[:1], -1));\n",
    "print(train_images_array.shape);\n",
    "\n",
    "#flatten test images array;\n",
    "test_images_array = test_images_array.reshape((*test_images_array.shape[:1], -1));\n",
    "print(test_images_array.shape);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One output node trainning => it wouldn't actually work for we would update the same set weights, therefore the last number(image) predicted would set the weights to recognize those specific patterns. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One output node for each of our numbers i.e. 0 to 9;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights = np.random.rand(784, 10);\n",
    "# alpha = .00000000001;\n",
    "# goal_pred = [];\n",
    "\n",
    "# def goal_pred_by_label(label):\n",
    "#     return {\n",
    "#          0: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "#          1: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "#          2: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "#          3: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
    "#          4: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "#          5: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
    "#          6: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
    "#          7: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
    "#          8: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
    "#          9: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
    "#     }.get(label);\n",
    "\n",
    "# for i in range(len(images_array)):\n",
    "#     goal_pred.append(goal_pred_by_label(labels_array[i]));\n",
    "    \n",
    "# def calc_delta(pred, labels_array):\n",
    "#     goal_pred, delta = [[], []];\n",
    "#     for i in range(len(labels_array)):\n",
    "#         goal_pred.append(goal_pred_by_label(labels_array[i]));\n",
    "#     for j in range(len(goal_pred)):\n",
    "#         delta.append(np.subtract(pred[j], goal_pred[j]));\n",
    "#     return delta;\n",
    "        \n",
    "# def neural_network(input, weights):\n",
    "#     for i in range(1):\n",
    "#         pred = np.dot(input, weights);\n",
    "#         #print('Weight: ' + str(weights[0]));\n",
    "#         msquared_error = (np.subtract(pred, goal_pred)) ** 2;\n",
    "#         delta = np.subtract(pred, goal_pred);\n",
    "#         weight_delta = np.dot(delta.T, input).T;\n",
    "#         #print('Weight_delta: ' + str(weight_delta[0]));\n",
    "#         #print('Error: ' + str(msquared_error[0]) + '\\n-----------------------------');\n",
    "#         weights = np.subtract(weights, (weight_delta * alpha));\n",
    "#         if(i == 99999 or i == 99998 or i == 99997 or i == 99996):\n",
    "#             print('Error: ' + str(msquared_error[0]) + '\\n-----------------------------');\n",
    "#         #plot_it_all(weights, msquared_error, weight_delta);\n",
    "# #         print('Label: ' + str(labels_array[0]) + ' \\nError: ' + str(msquared_error[0])\n",
    "# #              + '\\nPred: ' + str(pred[0]) + '\\nGoal_pred: ' + str(goal_pred[0]) \n",
    "# #              + '\\n-----------------------------');\n",
    "\n",
    "# def plot_it_all(weights, errors, derivatives):\n",
    "    \n",
    "#     ax1.set_title('How much changing each weight' \n",
    "#                   + '\\n contributed to the error?');\n",
    "#     ax1.set_ylabel('Mean squared error');\n",
    "#     ax1.set_xlabel('Weight');\n",
    "#     ax1.scatter(weights, errors, s=None, c='g');\n",
    "#     ax1.plot(weights, errors);\n",
    "#     for i in range(len(weights)):\n",
    "#         ax1.annotate(i, (weights[i], errors[i]));\n",
    "\n",
    "# neural_network(images_array, weights);7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MNIST stochastic gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(1);\n",
    "\n",
    "alpha, iterations, hidden_size = (.001, 500, 400); \n",
    "pixels_per_image, num_of_labels = (784, 10);\n",
    "\n",
    "synapse_0 = .2 * np.random.random((pixels_per_image, hidden_size)) - .1;\n",
    "synapse_1 = .2 * np.random.random((hidden_size, num_of_labels)) - .1;\n",
    "\n",
    "relu = lambda x:(x > 0) * x;\n",
    "relu2deriv = lambda x:(x > 0);\n",
    "\n",
    "for iteration in range(iterations):\n",
    "    msquared_error_layer_2, correct_cnt = (0.0, 0);\n",
    "    for index in range(len(train_images_array)):\n",
    "        #forward propagation\n",
    "        layer_0 = train_images_array[index:index+1];\n",
    "        layer_1 = relu(layer_0.dot(synapse_0));\n",
    "        #dropout\n",
    "        dropout_mask = np.random.randint(2, size=layer_1.shape);\n",
    "        layer_1 *= dropout_mask * 2;\n",
    "        \n",
    "        layer_2 = layer_1.dot(synapse_1);#l_2 = relu(l_0S_0)S_1;\n",
    "        #print(layer_2);\n",
    "        msquared_error_layer_2 += np.sum((layer_2 \n",
    "                - train_labels_array[index:index+1]) ** 2);\n",
    "        \n",
    "        correct_cnt += int(np.argmax(layer_2) == np.argmax(train_labels_array[index:index+1]));\n",
    "        \n",
    "        layer_2_delta = layer_2 - train_labels_array[index:index+1];\n",
    "        \n",
    "        layer_1_delta = layer_2_delta.dot(synapse_1.T) * relu2deriv(layer_1);\n",
    "        layer_1_delta *= dropout_mask;\n",
    "        \n",
    "        synapse_1_delta = layer_1.T.dot(layer_2_delta);\n",
    "        synapse_0_delta = layer_0.T.dot(layer_1_delta);\n",
    "        \n",
    "        synapse_1 -= synapse_1_delta * alpha;\n",
    "        synapse_0 -= synapse_0_delta * alpha;\n",
    "    #time for inference    \n",
    "    if(iteration % 10 == 0):\n",
    "        #page -r msquared_error_layer_2\n",
    "        #print(msquared_error_layer_2);\n",
    "        #time for inference \n",
    "        msquared_error_test, correct_cnt_test = (0.0, 0.0);\n",
    "        for index in range(len(test_images_array)):\n",
    "            layer_0 = test_images_array[index:index+1];\n",
    "            layer_1 = relu(layer_0.dot(synapse_0));\n",
    "            layer_2 = layer_1.dot(synapse_1);\n",
    "    \n",
    "            msquared_error_test += np.sum((layer_2 - test_labels_array[index:index+1]) ** 2);\n",
    "            correct_cnt_test += int(np.argmax(layer_2) == np.argmax(test_labels_array[index:index+1]));\n",
    "            \n",
    "        sys.stdout.write(\"\\n\"\n",
    "                             + \"I:\" + str(iteration)\n",
    "                             + \" Train Error:\" + str(msquared_error_layer_2/float(len(train_images_array)))[0:5]\n",
    "                             + \" Train Correct:\" + str(correct_cnt/len(train_images_array))\n",
    "                             + \" Test Error: \" + str(msquared_error_test/len(test_images_array))[0:5]\n",
    "                              +\" Test Correct: \" + str(correct_cnt_test/len(test_images_array)));\n",
    "                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini-bitched stochastic gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "I:0 Train Error:41.00 Train Correct:0.109 Test Error: 26.55 Test Correct: 0.185\n",
      "I:10 Train Error:23.08 Train Correct:0.313 Test Error: 22.10 Test Correct: 0.346\n",
      "I:20 Train Error:19.88 Train Correct:0.399 Test Error: 20.05 Test Correct: 0.394\n",
      "I:30 Train Error:18.36 Train Correct:0.417 Test Error: 18.59 Test Correct: 0.428\n",
      "I:40 Train Error:17.09 Train Correct:0.436 Test Error: 17.52 Test Correct: 0.452\n",
      "I:50 Train Error:16.06 Train Correct:0.469 Test Error: 16.67 Test Correct: 0.473\n",
      "I:60 Train Error:15.39 Train Correct:0.486 Test Error: 16.00 Test Correct: 0.491\n",
      "I:70 Train Error:14.63 Train Correct:0.505 Test Error: 15.47 Test Correct: 0.501\n",
      "I:80 Train Error:14.00 Train Correct:0.509 Test Error: 15.01 Test Correct: 0.503\n",
      "I:90 Train Error:13.49 Train Correct:0.529 Test Error: 14.59 Test Correct: 0.523\n",
      "I:100 Train Error:13.08 Train Correct:0.53 Test Error: 14.24 Test Correct: 0.522\n",
      "I:110 Train Error:12.67 Train Correct:0.555 Test Error: 13.89 Test Correct: 0.527\n",
      "I:120 Train Error:12.61 Train Correct:0.534 Test Error: 13.56 Test Correct: 0.535\n",
      "I:130 Train Error:12.09 Train Correct:0.556 Test Error: 13.31 Test Correct: 0.54\n",
      "I:140 Train Error:11.70 Train Correct:0.575 Test Error: 13.09 Test Correct: 0.547\n",
      "I:150 Train Error:11.73 Train Correct:0.583 Test Error: 12.82 Test Correct: 0.544\n",
      "I:160 Train Error:11.22 Train Correct:0.578 Test Error: 12.61 Test Correct: 0.554\n",
      "I:170 Train Error:11.01 Train Correct:0.562 Test Error: 12.43 Test Correct: 0.557\n",
      "I:180 Train Error:10.71 Train Correct:0.609 Test Error: 12.23 Test Correct: 0.557\n",
      "I:190 Train Error:10.66 Train Correct:0.597 Test Error: 12.04 Test Correct: 0.561\n",
      "I:200 Train Error:10.60 Train Correct:0.606 Test Error: 11.93 Test Correct: 0.567\n",
      "I:210 Train Error:10.26 Train Correct:0.6 Test Error: 11.74 Test Correct: 0.566\n",
      "I:220 Train Error:10.29 Train Correct:0.608 Test Error: 11.60 Test Correct: 0.574\n",
      "I:230 Train Error:9.860 Train Correct:0.617 Test Error: 11.41 Test Correct: 0.58\n",
      "I:240 Train Error:9.757 Train Correct:0.633 Test Error: 11.28 Test Correct: 0.577\n",
      "I:250 Train Error:9.625 Train Correct:0.625 Test Error: 11.17 Test Correct: 0.584\n",
      "I:260 Train Error:9.555 Train Correct:0.624 Test Error: 11.07 Test Correct: 0.586\n",
      "I:270 Train Error:9.248 Train Correct:0.648 Test Error: 10.91 Test Correct: 0.591\n",
      "I:280 Train Error:9.331 Train Correct:0.641 Test Error: 10.78 Test Correct: 0.588\n",
      "I:290 Train Error:9.052 Train Correct:0.623 Test Error: 10.71 Test Correct: 0.595\n",
      "I:300 Train Error:8.924 Train Correct:0.651 Test Error: 10.60 Test Correct: 0.599\n",
      "I:310 Train Error:8.958 Train Correct:0.642 Test Error: 10.48 Test Correct: 0.605\n",
      "I:320 Train Error:8.668 Train Correct:0.639 Test Error: 10.40 Test Correct: 0.603\n",
      "I:330 Train Error:8.599 Train Correct:0.651 Test Error: 10.34 Test Correct: 0.61\n",
      "I:340 Train Error:8.483 Train Correct:0.663 Test Error: 10.24 Test Correct: 0.615\n",
      "I:350 Train Error:8.605 Train Correct:0.653 Test Error: 10.15 Test Correct: 0.611\n",
      "I:360 Train Error:8.257 Train Correct:0.661 Test Error: 10.06 Test Correct: 0.617\n",
      "I:370 Train Error:8.275 Train Correct:0.667 Test Error: 10.00 Test Correct: 0.622\n",
      "I:380 Train Error:8.180 Train Correct:0.659 Test Error: 9.902 Test Correct: 0.626\n",
      "I:390 Train Error:8.203 Train Correct:0.659 Test Error: 9.857 Test Correct: 0.624\n",
      "I:400 Train Error:7.989 Train Correct:0.677 Test Error: 9.793 Test Correct: 0.63\n",
      "I:410 Train Error:7.927 Train Correct:0.666 Test Error: 9.743 Test Correct: 0.631\n",
      "I:420 Train Error:7.771 Train Correct:0.679 Test Error: 9.705 Test Correct: 0.635\n",
      "I:430 Train Error:7.969 Train Correct:0.675 Test Error: 9.578 Test Correct: 0.627\n",
      "I:440 Train Error:7.631 Train Correct:0.695 Test Error: 9.539 Test Correct: 0.634\n",
      "I:450 Train Error:7.661 Train Correct:0.689 Test Error: 9.449 Test Correct: 0.634\n",
      "I:460 Train Error:7.622 Train Correct:0.691 Test Error: 9.422 Test Correct: 0.637\n",
      "I:470 Train Error:7.347 Train Correct:0.693 Test Error: 9.314 Test Correct: 0.634\n",
      "I:480 Train Error:7.424 Train Correct:0.678 Test Error: 9.281 Test Correct: 0.638\n",
      "I:490 Train Error:7.308 Train Correct:0.689 Test Error: 9.250 Test Correct: 0.64"
     ]
    }
   ],
   "source": [
    "np.random.seed(1);\n",
    "\n",
    "batch_size = 100; \n",
    "alpha, iterations, hidden_size = (.001, 500, 400); \n",
    "pixels_per_image, num_of_labels = (784, 10);\n",
    "\n",
    "synapse_0 = .2 * np.random.random((pixels_per_image, hidden_size)) - .1;\n",
    "synapse_1 = .2 * np.random.random((hidden_size, num_of_labels)) - .1;\n",
    "\n",
    "relu = lambda x:(x > 0) * x;\n",
    "relu2deriv = lambda x:(x > 0);\n",
    "\n",
    "for iteration in range(iterations):\n",
    "    msquared_error_layer_2, correct_cnt = (0.0, 0);\n",
    "    for index in range(int(len(train_images_array)/batch_size)):\n",
    "        batch_start, batch_end = (index * batch_size, (index + 1) * batch_size);\n",
    "        #forward propagation\n",
    "        layer_0 = train_images_array[batch_start:batch_end];\n",
    "        layer_1 = relu(layer_0.dot(synapse_0));\n",
    "        #dropout\n",
    "        dropout_mask = np.random.randint(2, size=layer_1.shape);\n",
    "        layer_1 *= dropout_mask * 2;\n",
    "        \n",
    "        layer_2 = layer_1.dot(synapse_1);#l_2 = relu(l_0S_0)S_1;\n",
    "        #print(layer_2);100x10\n",
    "        msquared_error_layer_2 += np.sum((layer_2 \n",
    "                - train_labels_array[batch_start:batch_end]) ** 2);\n",
    "        \n",
    "        for index_cnt in range(batch_size):\n",
    "            correct_cnt += int(np.argmax(layer_2[index_cnt:index_cnt + 1]) == \\\n",
    "                                   np.argmax(train_labels_array[batch_start + index_cnt: batch_start + index_cnt + 1]));\n",
    "            \n",
    "        layer_2_delta = (layer_2 - train_labels_array[batch_start:batch_end]) \\\n",
    "                                        / batch_size;\n",
    "        \n",
    "        layer_1_delta = layer_2_delta.dot(synapse_1.T) * relu2deriv(layer_1);\n",
    "        #I`ve added the '*2' in the backward step for otherwise\n",
    "        #we would be computing a gradient of a different function\n",
    "        #than we`re evaluating.\n",
    "        Thus, generally, it`s important to account for anything we're doing in the forward \n",
    "        step, in the backward step as well.\n",
    "        layer_1_delta *= dropout_mask * 2;\n",
    "        \n",
    "        synapse_1_delta = layer_1.T.dot(layer_2_delta);\n",
    "        synapse_0_delta = layer_0.T.dot(layer_1_delta);\n",
    "        \n",
    "        synapse_1 -= synapse_1_delta * alpha;\n",
    "        synapse_0 -= synapse_0_delta * alpha;\n",
    "    #time for inference    \n",
    "    if(iteration % 10 == 0):\n",
    "        #page -r msquared_error_layer_2\n",
    "        #print(msquared_error_layer_2);\n",
    "        #time for inference \n",
    "        msquared_error_test, correct_cnt_test = (0.0, 0.0);\n",
    "        for index in range(len(test_images_array)):\n",
    "            layer_0 = test_images_array[index:index+1];\n",
    "            layer_1 = relu(layer_0.dot(synapse_0));\n",
    "            layer_2 = layer_1.dot(synapse_1);\n",
    "    \n",
    "            msquared_error_test += np.sum((layer_2 - test_labels_array[index:index+1]) ** 2);\n",
    "            correct_cnt_test += int(np.argmax(layer_2) == np.argmax(test_labels_array[index:index+1]));\n",
    "            \n",
    "        sys.stdout.write(\"\\n\"\n",
    "                             + \"I:\" + str(iteration)\n",
    "                             + \" Train Error:\" + str(msquared_error_layer_2/float(len(train_images_array)))[0:5]\n",
    "                             + \" Train Correct:\" + str(correct_cnt/len(train_images_array))\n",
    "                             + \" Test Error: \" + str(msquared_error_test/len(test_images_array))[0:5]\n",
    "                              +\" Test Correct: \" + str(correct_cnt_test/len(test_images_array)));\n",
    "                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-48-4f741ce8c3de>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-48-4f741ce8c3de>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    weights also can be seen as a high dimensional shape.\u001b[0m\n\u001b[0m               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "weights also can be seen as a high dimensional shape.\n",
    "As you train, this shape molds around your data,\n",
    "learning to distinguish one pattern from another.\n",
    "The images in our testing dataset were slightly different \n",
    "than the patterns in our train set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Modeling specific types of phenomenon in data. See you there!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
